{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Article_Generation_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU5IAXxdiULo"
      },
      "source": [
        "!pip install TensorFlow==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ_vxEVKiKxg"
      },
      "source": [
        "!pip3 install gpt-2-simple\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OUDU4zQiMBE",
        "outputId": "d1d6d616-696b-43ee-e1ac-873053809400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqMO1ZVUiSFm"
      },
      "source": [
        "model_name = \"117M\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsmOsrvHjNxp",
        "outputId": "2a17dd9f-94c2-483c-ae1b-362f8f61674e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 321Mit/s]                                                      "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 117M model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching encoder.json: 1.05Mit [00:00, 127Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 762Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:01, 257Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 223Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 168Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 210Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpNR-XQ1jhRD"
      },
      "source": [
        "sess = gpt2.start_tf_sess()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQyb5adxl_aF"
      },
      "source": [
        "file_name = \"train.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RP9OtXhkxEH"
      },
      "source": [
        "\n",
        "if not os.path.isfile(file_name):\n",
        "\turl = link\n",
        "\tdata = requests.get(url)\n",
        "\t\n",
        "\twith open(file_name, 'w') as f:\n",
        "\t\tf.write(data.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXFC5T2WjpYr",
        "outputId": "495f1d29-dbe7-4667-a4a8-428ac1fdf893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.finetune(sess,\n",
        "              file_name,\n",
        "              model_name=model_name,  \n",
        "              steps=1000, run_name = 'run1'\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 542387 tokens\n",
            "Training...\n",
            "[1 | 9.16] loss=3.42 avg=3.42\n",
            "[2 | 11.36] loss=3.16 avg=3.29\n",
            "[3 | 13.55] loss=3.45 avg=3.34\n",
            "[4 | 15.75] loss=3.34 avg=3.34\n",
            "[5 | 17.96] loss=3.55 avg=3.38\n",
            "[6 | 20.16] loss=3.10 avg=3.34\n",
            "[7 | 22.37] loss=3.27 avg=3.33\n",
            "[8 | 24.59] loss=3.95 avg=3.41\n",
            "[9 | 26.81] loss=3.25 avg=3.39\n",
            "[10 | 29.04] loss=3.41 avg=3.39\n",
            "[11 | 31.28] loss=3.34 avg=3.39\n",
            "[12 | 33.52] loss=3.46 avg=3.39\n",
            "[13 | 35.76] loss=3.15 avg=3.37\n",
            "[14 | 38.00] loss=3.44 avg=3.38\n",
            "[15 | 40.25] loss=3.29 avg=3.37\n",
            "[16 | 42.50] loss=3.09 avg=3.35\n",
            "[17 | 44.76] loss=3.39 avg=3.36\n",
            "[18 | 47.02] loss=3.14 avg=3.34\n",
            "[19 | 49.28] loss=3.35 avg=3.34\n",
            "[20 | 51.55] loss=2.90 avg=3.32\n",
            "[21 | 53.81] loss=3.27 avg=3.32\n",
            "[22 | 56.08] loss=3.36 avg=3.32\n",
            "[23 | 58.34] loss=3.08 avg=3.31\n",
            "[24 | 60.59] loss=3.28 avg=3.30\n",
            "[25 | 62.84] loss=3.16 avg=3.30\n",
            "[26 | 65.08] loss=3.30 avg=3.30\n",
            "[27 | 67.33] loss=3.07 avg=3.29\n",
            "[28 | 69.57] loss=2.99 avg=3.28\n",
            "[29 | 71.82] loss=3.00 avg=3.27\n",
            "[30 | 74.06] loss=3.19 avg=3.26\n",
            "[31 | 76.30] loss=3.05 avg=3.25\n",
            "[32 | 78.54] loss=2.87 avg=3.24\n",
            "[33 | 80.78] loss=3.26 avg=3.24\n",
            "[34 | 83.01] loss=2.62 avg=3.22\n",
            "[35 | 85.25] loss=3.12 avg=3.22\n",
            "[36 | 87.48] loss=2.95 avg=3.21\n",
            "[37 | 89.71] loss=3.33 avg=3.21\n",
            "[38 | 91.94] loss=3.06 avg=3.21\n",
            "[39 | 94.18] loss=3.58 avg=3.22\n",
            "[40 | 96.42] loss=3.26 avg=3.22\n",
            "[41 | 98.65] loss=3.02 avg=3.21\n",
            "[42 | 100.88] loss=3.28 avg=3.22\n",
            "[43 | 103.12] loss=3.20 avg=3.22\n",
            "[44 | 105.36] loss=3.24 avg=3.22\n",
            "[45 | 107.58] loss=3.14 avg=3.21\n",
            "[46 | 109.82] loss=2.58 avg=3.20\n",
            "[47 | 112.07] loss=3.14 avg=3.20\n",
            "[48 | 114.31] loss=3.09 avg=3.19\n",
            "[49 | 116.57] loss=3.32 avg=3.20\n",
            "[50 | 118.81] loss=3.06 avg=3.19\n",
            "[51 | 121.06] loss=3.13 avg=3.19\n",
            "[52 | 123.31] loss=3.16 avg=3.19\n",
            "[53 | 125.56] loss=3.10 avg=3.19\n",
            "[54 | 127.81] loss=2.93 avg=3.18\n",
            "[55 | 130.06] loss=3.42 avg=3.19\n",
            "[56 | 132.31] loss=2.94 avg=3.18\n",
            "[57 | 134.56] loss=2.98 avg=3.18\n",
            "[58 | 136.81] loss=3.03 avg=3.17\n",
            "[59 | 139.07] loss=3.03 avg=3.17\n",
            "[60 | 141.31] loss=3.12 avg=3.17\n",
            "[61 | 143.56] loss=3.16 avg=3.17\n",
            "[62 | 145.81] loss=3.22 avg=3.17\n",
            "[63 | 148.06] loss=3.12 avg=3.17\n",
            "[64 | 150.31] loss=3.02 avg=3.17\n",
            "[65 | 152.56] loss=3.12 avg=3.17\n",
            "[66 | 154.81] loss=3.37 avg=3.17\n",
            "[67 | 157.06] loss=2.78 avg=3.16\n",
            "[68 | 159.31] loss=2.88 avg=3.16\n",
            "[69 | 161.56] loss=3.17 avg=3.16\n",
            "[70 | 163.80] loss=2.97 avg=3.15\n",
            "[71 | 166.07] loss=3.24 avg=3.15\n",
            "[72 | 168.32] loss=3.01 avg=3.15\n",
            "[73 | 170.56] loss=2.78 avg=3.14\n",
            "[74 | 172.80] loss=3.14 avg=3.14\n",
            "[75 | 175.05] loss=2.87 avg=3.14\n",
            "[76 | 177.29] loss=2.51 avg=3.13\n",
            "[77 | 179.54] loss=2.96 avg=3.12\n",
            "[78 | 181.80] loss=2.95 avg=3.12\n",
            "[79 | 184.04] loss=3.12 avg=3.12\n",
            "[80 | 186.29] loss=2.77 avg=3.11\n",
            "[81 | 188.53] loss=3.04 avg=3.11\n",
            "[82 | 190.77] loss=3.11 avg=3.11\n",
            "[83 | 193.01] loss=3.19 avg=3.11\n",
            "[84 | 195.25] loss=3.04 avg=3.11\n",
            "[85 | 197.49] loss=2.97 avg=3.11\n",
            "[86 | 199.73] loss=3.01 avg=3.11\n",
            "[87 | 201.98] loss=3.12 avg=3.11\n",
            "[88 | 204.22] loss=2.92 avg=3.11\n",
            "[89 | 206.47] loss=3.01 avg=3.10\n",
            "[90 | 208.71] loss=2.85 avg=3.10\n",
            "[91 | 210.95] loss=3.10 avg=3.10\n",
            "[92 | 213.20] loss=2.53 avg=3.09\n",
            "[93 | 215.44] loss=2.96 avg=3.09\n",
            "[94 | 217.69] loss=3.43 avg=3.09\n",
            "[95 | 219.93] loss=3.09 avg=3.09\n",
            "[96 | 222.17] loss=3.29 avg=3.10\n",
            "[97 | 224.42] loss=3.16 avg=3.10\n",
            "[98 | 226.67] loss=3.10 avg=3.10\n",
            "[99 | 228.91] loss=2.96 avg=3.10\n",
            "[100 | 231.15] loss=2.59 avg=3.09\n",
            "======== SAMPLE 1 ========\n",
            "ฝงตวกรขดนคดเงยย่พทขนวกกเงยร่พ\n",
            "\n",
            "สสนควไกตวกร่าดม่ทัคกขกไกร่์ขแนดบ็กั่คกร จพ่คทกต ็กั่คกไกร ็กั่คกร จพ่คเงยร่พ ใแปเงยย่ คกกร ขกงผ ถกอก คกร  ใคกๆ กร แลค กซ แกก แงจกค฀ มๅ ย่พ ชะ กงขขทเง ฏกซ เง ฏกค แกก แกกโท ็กก แกก ซ กลคก กก แก แก ซ กลค กซ แก แก แกคกท กก คกก กก ง กกโทเง วกกช ซ ไกไ กโท ็กโทเ ษ เง เง ฌ ็กแ ��� ็ โทง اوَ คกก  คก กก คก โทเง วก กพ ใคก แกก แกก กโท อก แก ต็ โคก ขคก ขง ซ กร จพ ก จพ ใคก แก แก น จพ ก แก แก ท ก ง กกแ ท  ค ก กก ช ซ กคก ก ก ก แก ใขน ต็ ห ข ซ  ไ ก ก ก �� ก ก แ ก ข ค ก ก ก ก แก ซ  ไกไ ก ก แก ก ไ แก ซ ก แก ข ค ก ก ก ก  ข ล จ ก  ก ه ก تเง า ร จพ ก จพ ใค ก ก  แก ก ก له ก ु ง ก แ จ ก แก ก  ก\n",
            "\n",
            "[101 | 245.98] loss=3.53 avg=3.09\n",
            "[102 | 248.22] loss=2.59 avg=3.09\n",
            "[103 | 250.47] loss=3.05 avg=3.09\n",
            "[104 | 252.72] loss=2.78 avg=3.08\n",
            "[105 | 254.96] loss=3.09 avg=3.08\n",
            "[106 | 257.20] loss=2.73 avg=3.08\n",
            "[107 | 259.44] loss=2.97 avg=3.07\n",
            "[108 | 261.69] loss=3.11 avg=3.08\n",
            "[109 | 263.94] loss=3.03 avg=3.07\n",
            "[110 | 266.18] loss=2.68 avg=3.07\n",
            "[111 | 268.42] loss=3.10 avg=3.07\n",
            "[112 | 270.66] loss=3.00 avg=3.07\n",
            "[113 | 272.91] loss=3.15 avg=3.07\n",
            "[114 | 275.16] loss=3.03 avg=3.07\n",
            "[115 | 277.40] loss=2.95 avg=3.07\n",
            "[116 | 279.65] loss=3.14 avg=3.07\n",
            "[117 | 281.90] loss=2.86 avg=3.07\n",
            "[118 | 284.14] loss=2.92 avg=3.06\n",
            "[119 | 286.38] loss=3.06 avg=3.06\n",
            "[120 | 288.63] loss=3.05 avg=3.06\n",
            "[121 | 290.87] loss=2.96 avg=3.06\n",
            "[122 | 293.12] loss=2.74 avg=3.06\n",
            "[123 | 295.36] loss=2.98 avg=3.06\n",
            "[124 | 297.61] loss=2.81 avg=3.05\n",
            "[125 | 299.85] loss=2.92 avg=3.05\n",
            "[126 | 302.10] loss=2.70 avg=3.05\n",
            "[127 | 304.35] loss=2.82 avg=3.04\n",
            "[128 | 306.59] loss=3.11 avg=3.04\n",
            "[129 | 308.84] loss=2.93 avg=3.04\n",
            "[130 | 311.09] loss=2.80 avg=3.04\n",
            "[131 | 313.34] loss=2.82 avg=3.04\n",
            "[132 | 315.59] loss=2.99 avg=3.03\n",
            "[133 | 317.84] loss=2.77 avg=3.03\n",
            "[134 | 320.08] loss=3.04 avg=3.03\n",
            "[135 | 322.33] loss=2.98 avg=3.03\n",
            "[136 | 324.58] loss=2.73 avg=3.03\n",
            "[137 | 326.83] loss=2.83 avg=3.02\n",
            "[138 | 329.08] loss=2.93 avg=3.02\n",
            "[139 | 331.33] loss=3.09 avg=3.02\n",
            "[140 | 333.57] loss=2.93 avg=3.02\n",
            "[141 | 335.82] loss=2.95 avg=3.02\n",
            "[142 | 338.07] loss=2.84 avg=3.02\n",
            "[143 | 340.32] loss=2.79 avg=3.02\n",
            "[144 | 342.56] loss=3.13 avg=3.02\n",
            "[145 | 344.81] loss=2.78 avg=3.01\n",
            "[146 | 347.06] loss=2.87 avg=3.01\n",
            "[147 | 349.31] loss=2.88 avg=3.01\n",
            "[148 | 351.55] loss=2.87 avg=3.01\n",
            "[149 | 353.80] loss=2.91 avg=3.01\n",
            "[150 | 356.05] loss=2.96 avg=3.01\n",
            "[151 | 358.30] loss=2.80 avg=3.00\n",
            "[152 | 360.55] loss=2.90 avg=3.00\n",
            "[153 | 362.80] loss=2.55 avg=3.00\n",
            "[154 | 365.05] loss=2.71 avg=2.99\n",
            "[155 | 367.30] loss=2.50 avg=2.99\n",
            "[156 | 369.55] loss=3.17 avg=2.99\n",
            "[157 | 371.80] loss=2.64 avg=2.99\n",
            "[158 | 374.05] loss=2.73 avg=2.98\n",
            "[159 | 376.30] loss=2.90 avg=2.98\n",
            "[160 | 378.55] loss=2.90 avg=2.98\n",
            "[161 | 380.79] loss=2.57 avg=2.98\n",
            "[162 | 383.03] loss=2.68 avg=2.97\n",
            "[163 | 385.28] loss=2.83 avg=2.97\n",
            "[164 | 387.54] loss=2.74 avg=2.97\n",
            "[165 | 389.78] loss=2.78 avg=2.96\n",
            "[166 | 392.03] loss=2.97 avg=2.96\n",
            "[167 | 394.28] loss=2.91 avg=2.96\n",
            "[168 | 396.52] loss=2.59 avg=2.96\n",
            "[169 | 398.78] loss=3.25 avg=2.96\n",
            "[170 | 401.03] loss=2.70 avg=2.96\n",
            "[171 | 403.27] loss=2.74 avg=2.96\n",
            "[172 | 405.52] loss=3.03 avg=2.96\n",
            "[173 | 407.77] loss=2.68 avg=2.95\n",
            "[174 | 410.02] loss=3.02 avg=2.96\n",
            "[175 | 412.27] loss=2.97 avg=2.96\n",
            "[176 | 414.51] loss=2.46 avg=2.95\n",
            "[177 | 416.76] loss=2.66 avg=2.95\n",
            "[178 | 419.00] loss=2.39 avg=2.94\n",
            "[179 | 421.24] loss=2.67 avg=2.94\n",
            "[180 | 423.49] loss=3.03 avg=2.94\n",
            "[181 | 425.74] loss=2.92 avg=2.94\n",
            "[182 | 427.98] loss=2.85 avg=2.94\n",
            "[183 | 430.23] loss=3.00 avg=2.94\n",
            "[184 | 432.48] loss=2.53 avg=2.93\n",
            "[185 | 434.73] loss=2.51 avg=2.93\n",
            "[186 | 436.98] loss=2.95 avg=2.93\n",
            "[187 | 439.22] loss=2.96 avg=2.93\n",
            "[188 | 441.47] loss=2.77 avg=2.93\n",
            "[189 | 443.72] loss=3.27 avg=2.93\n",
            "[190 | 445.97] loss=2.71 avg=2.93\n",
            "[191 | 448.22] loss=2.94 avg=2.93\n",
            "[192 | 450.48] loss=2.61 avg=2.92\n",
            "[193 | 452.72] loss=2.79 avg=2.92\n",
            "[194 | 454.97] loss=2.79 avg=2.92\n",
            "[195 | 457.21] loss=2.79 avg=2.92\n",
            "[196 | 459.46] loss=2.50 avg=2.91\n",
            "[197 | 461.71] loss=3.19 avg=2.92\n",
            "[198 | 463.95] loss=2.98 avg=2.92\n",
            "[199 | 466.20] loss=2.66 avg=2.92\n",
            "[200 | 468.44] loss=2.59 avg=2.91\n",
            "======== SAMPLE 1 ========\n",
            "�वर\\u300f_’s’s Natural Language Processing Kit. As you can see, the entire model runs in one go. The code is quite simple. The first test is to identify the input and it will match the input with the given label. The second test is to identify the output. This test is to do so that the model can perform further evaluation on the output of a model that is given an input message that is not the full output.', 'This is where the concept of tokenization come in handy in our language generation. For example:', 'Now let’s see how this tokenization works. On the left hand side is the list of tokens to be stored on disk. On the right hand side is the data that will be used to generate the tokens. It basically says:', 'On the right is the model to train. All of the train data can be read from disk, and the model to output to the GPU is going to be given.', 'In order to perform further evaluation, the model we just trained on is going to be fed into a neural network to predict the labels or tokens in the given input. It is really just looking at the data on the left hand side. Notice how the output is always exactly the same, which makes it an efficient machine learning metric. Also, if the model is trained on all the input, the model should be able to infer the label from the input — at which point it will automatically infer either the full or the hidden data into the model. The output is the model trained on to output. Notice how the input information is always the same and that it is always an output. But if the model is trained on only the hidden data then it is not able to correctly infer the label from the output.', 'In order to address this issue, we want to learn a few techniques for dealing with residuals, which is a class of residuals in machine learning. The residuals in the model are a set of latent features that are a function of the input data, which are latent, non-linear and recurrent features. Here are a couple of them:', 'These residuals do not depend on the model and, in general, are not continuous. This means that they are always latent. Remember, the problem is that we are trying to estimate the magnitude of the residual. You are not, however, able to measure the linearity of residuals, which has limits on what we may be able to measure. In particular, the problem is that there is no direct relationship between the latent feature size and the residual we are trying to get.', 'Here is the first problem we are going to solve: it is necessary to have the latent feature size in the model to determine the residual. The model only needs to be trained on the latent feature. And how we would perform this is not explained in the first part.', 'This is where the concept of self-attention comes in handy. The model uses as good an attention mechanism as possible when you are solving this problem. As shown in the image below, the model is trained on the inputs and output and the input model is only trained on the output. We are not going to use it in this example. But what I would try to mention is that you can also use self-attention as a way to train on your input, which is basically a set of latent features. And this is where we get the term ‘s’ for deep learning.', 'Now let’s see how this works. The term ‘s’ is a representation of the input as a hidden value that is a function of the inputs.', 'On the left we get that term the word s.', 'On the right we get that term the word ‘s’. And finally it is ‘n’, which is a very different term from the term ‘s’:', 'In general, the term is also a representation of the input. So here we have all the term represented as a representation.', 'We have all the term represented as a term.', 'Notice how we have one term that is only represented as a term and a term that is only represented as the sentence n.', 'This is the concept of embedding. This is basically a kind of self-attention mechanism that is used to get the input representation of the term. The model is trained to predict the term within the embedding space. And the resulting embedding space is what is referred to as a hidden dimension space.', 'This is a key reason why we use attention.', 'Here a sentence can be represented as a term embedding as a network that is trained over the input to produce a fixed term.', 'There are two different ways to encode a term:', 'The first way is to encode the word using a pre-trained attention mechanism. This means that the attention mechanism will train the model and\n",
            "\n",
            "[201 | 482.11] loss=2.47 avg=2.91\n",
            "[202 | 484.35] loss=2.51 avg=2.90\n",
            "[203 | 486.60] loss=2.80 avg=2.90\n",
            "[204 | 488.84] loss=3.04 avg=2.90\n",
            "[205 | 491.08] loss=2.69 avg=2.90\n",
            "[206 | 493.32] loss=2.88 avg=2.90\n",
            "[207 | 495.56] loss=2.57 avg=2.90\n",
            "[208 | 497.81] loss=2.65 avg=2.89\n",
            "[209 | 500.05] loss=2.51 avg=2.89\n",
            "[210 | 502.30] loss=2.53 avg=2.88\n",
            "[211 | 504.55] loss=2.56 avg=2.88\n",
            "[212 | 506.79] loss=2.95 avg=2.88\n",
            "[213 | 509.03] loss=2.44 avg=2.88\n",
            "[214 | 511.28] loss=2.96 avg=2.88\n",
            "[215 | 513.53] loss=2.68 avg=2.88\n",
            "[216 | 515.77] loss=3.13 avg=2.88\n",
            "[217 | 518.02] loss=3.06 avg=2.88\n",
            "[218 | 520.27] loss=2.78 avg=2.88\n",
            "[219 | 522.52] loss=2.79 avg=2.88\n",
            "[220 | 524.76] loss=2.74 avg=2.88\n",
            "[221 | 527.01] loss=2.62 avg=2.87\n",
            "[222 | 529.25] loss=2.67 avg=2.87\n",
            "[223 | 531.49] loss=2.87 avg=2.87\n",
            "[224 | 533.74] loss=2.75 avg=2.87\n",
            "[225 | 535.99] loss=2.68 avg=2.87\n",
            "[226 | 538.23] loss=2.47 avg=2.86\n",
            "[227 | 540.47] loss=2.68 avg=2.86\n",
            "[228 | 542.72] loss=2.80 avg=2.86\n",
            "[229 | 544.96] loss=2.42 avg=2.86\n",
            "[230 | 547.21] loss=2.75 avg=2.85\n",
            "[231 | 549.46] loss=2.79 avg=2.85\n",
            "[232 | 551.70] loss=2.57 avg=2.85\n",
            "[233 | 553.95] loss=2.51 avg=2.85\n",
            "[234 | 556.19] loss=2.81 avg=2.85\n",
            "[235 | 558.45] loss=2.64 avg=2.84\n",
            "[236 | 560.69] loss=2.57 avg=2.84\n",
            "[237 | 562.94] loss=2.74 avg=2.84\n",
            "[238 | 565.18] loss=2.33 avg=2.83\n",
            "[239 | 567.42] loss=2.32 avg=2.83\n",
            "[240 | 569.67] loss=3.01 avg=2.83\n",
            "[241 | 571.91] loss=2.90 avg=2.83\n",
            "[242 | 574.16] loss=2.82 avg=2.83\n",
            "[243 | 576.40] loss=2.80 avg=2.83\n",
            "[244 | 578.65] loss=2.55 avg=2.83\n",
            "[245 | 580.89] loss=2.64 avg=2.83\n",
            "[246 | 583.14] loss=2.61 avg=2.82\n",
            "[247 | 585.38] loss=2.62 avg=2.82\n",
            "[248 | 587.62] loss=2.77 avg=2.82\n",
            "[249 | 589.86] loss=2.77 avg=2.82\n",
            "[250 | 592.10] loss=2.54 avg=2.82\n",
            "[251 | 594.34] loss=2.64 avg=2.82\n",
            "[252 | 596.59] loss=2.68 avg=2.81\n",
            "[253 | 598.84] loss=2.67 avg=2.81\n",
            "[254 | 601.08] loss=2.54 avg=2.81\n",
            "[255 | 603.33] loss=2.52 avg=2.81\n",
            "[256 | 605.57] loss=2.88 avg=2.81\n",
            "[257 | 607.81] loss=2.50 avg=2.80\n",
            "[258 | 610.05] loss=2.47 avg=2.80\n",
            "[259 | 612.30] loss=2.59 avg=2.80\n",
            "[260 | 614.55] loss=2.39 avg=2.79\n",
            "[261 | 616.80] loss=2.83 avg=2.79\n",
            "[262 | 619.04] loss=2.51 avg=2.79\n",
            "[263 | 621.28] loss=2.42 avg=2.79\n",
            "[264 | 623.52] loss=2.74 avg=2.79\n",
            "[265 | 625.77] loss=2.81 avg=2.79\n",
            "[266 | 628.03] loss=2.34 avg=2.78\n",
            "[267 | 630.27] loss=2.93 avg=2.78\n",
            "[268 | 632.52] loss=2.28 avg=2.78\n",
            "[269 | 634.77] loss=2.92 avg=2.78\n",
            "[270 | 637.00] loss=2.13 avg=2.77\n",
            "[271 | 639.25] loss=2.07 avg=2.77\n",
            "[272 | 641.49] loss=2.42 avg=2.76\n",
            "[273 | 643.73] loss=2.65 avg=2.76\n",
            "[274 | 645.98] loss=2.45 avg=2.76\n",
            "[275 | 648.23] loss=2.60 avg=2.76\n",
            "[276 | 650.48] loss=2.56 avg=2.75\n",
            "[277 | 652.72] loss=2.75 avg=2.75\n",
            "[278 | 654.97] loss=2.29 avg=2.75\n",
            "[279 | 657.22] loss=2.50 avg=2.75\n",
            "[280 | 659.47] loss=2.64 avg=2.74\n",
            "[281 | 661.71] loss=2.68 avg=2.74\n",
            "[282 | 663.96] loss=2.63 avg=2.74\n",
            "[283 | 666.20] loss=2.70 avg=2.74\n",
            "[284 | 668.44] loss=2.68 avg=2.74\n",
            "[285 | 670.68] loss=2.80 avg=2.74\n",
            "[286 | 672.93] loss=2.29 avg=2.74\n",
            "[287 | 675.17] loss=2.47 avg=2.73\n",
            "[288 | 677.41] loss=2.30 avg=2.73\n",
            "[289 | 679.66] loss=2.37 avg=2.73\n",
            "[290 | 681.89] loss=2.37 avg=2.72\n",
            "[291 | 684.14] loss=2.58 avg=2.72\n",
            "[292 | 686.38] loss=2.59 avg=2.72\n",
            "[293 | 688.62] loss=2.70 avg=2.72\n",
            "[294 | 690.87] loss=2.53 avg=2.72\n",
            "[295 | 693.11] loss=2.44 avg=2.71\n",
            "[296 | 695.36] loss=2.42 avg=2.71\n",
            "[297 | 697.61] loss=2.37 avg=2.71\n",
            "[298 | 699.86] loss=2.74 avg=2.71\n",
            "[299 | 702.11] loss=2.64 avg=2.71\n",
            "[300 | 704.35] loss=2.62 avg=2.71\n",
            "======== SAMPLE 1 ========\n",
            ". It is also possible to check that a given word is in the right context without the incorrect context.', 'How about a more efficient search engine to find specific words in a given context, then combining that with a word as a query? This would be a totally different experience. That is to say, I search my database searching for a specific word and I find and extract a specific word from the database. This could be a very compelling search strategy that doesn’t require computing resources. What about building a system that can recognize and extract the context of specific contexts that are being searched for? For example, a social network might be able to tell if a phrase in a profile shared by one friend is a specific person. That would be incredibly helpful in achieving search results but it would require a lot of computing power of some kind.', 'We’ll explore these possibilities in Part II of our series on Artificial Genomes.', 'Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual.\\xa0Learn more', 'Create a free Medium account to get The Daily Pick in your inbox.', 'Written by', 'Written by'] ['This is the second part of a two-part series on NLP: Transformer with Transformer Modelling (TSM). Transformer with transformer modelling (TF-TOM) is the key ingredient of NLP that will transform any language into a sequence of sentence. The TF-TOM is a transformational neural-level transformer model which will allow you to transform any language into “sequence” (or sequence), even though it does not capture the full meaning of words and the semantics of words is limited. It should be noted that transformational neural-level techniques will not capture the full sequence of words within text — they do not capture the whole sequence of words in the final output. Instead, they capture word representations of words that will allow our NLP model to learn from this sequence of word representations instead of from the entire sequence of word representations in the original word. The next part of this series will be about how TF-TOM can also be used for non-linear NLP tasks: In particular “transformational ML”- the simplest way to transform a language into sequence of words. Since the transformational ML uses the transformer’s context-free transformer encoder to output sequence of word representations of words, it can be easily transformed into a sequence of words and be easily processed and reused to make “sequence” representations of words in a sequence. These form the backbone of NLP where words occur in multiple sequence in a N-loop (tokenized) and in a regular N-loop to a sequence of words across a sequence where the N is one. This simple approach helps to train NLP models for any NLP task with no special processing. An example of how TTM can be easily converted into sequence of words might be given in Figure 2. Figure 2: Simple TF-TOM model using TF-TOM, TF-CLS, and T5.', 'To learn more about TF-TOM, see a detailed blog post. To read an interactive story on NLP with Tomsen, see our story.', 'This article will be a part of the series that will introduce you to the Transformers’ approach that will transform Word2Vec into sequence of word representations from their context-free transformer model. Transformers with Transformer Modelling (TF-TOM) is based on Transformer’s previous work to explore the relationship between word context and language structure which has led to a number of papers (here, here, and here).', 'The Transformer’s simple approach to language modelling is a very promising candidate for NLP with a high degree of generality. The objective of this article is to show you how to implement our TF-TOM model for word embeddings extracted from text into Word2Vec format.', 'It is important to notice that the model consists of only two tasks: the task of word embeddings is done in Word2Vec format.', 'The purpose of this article is to show how the TF-TOM approach can be applied to Word2Vec which can yield a rich vocabularies. In addition, it uses the same transformer for Word2Vec generated vocabularies of the same name as shown below:', 'This article will be divided into the following sections. The content for each section is in it’s entirety and most of the information is relevant within the text used for inference. There are very few sentences that’ll be shown as ‘lengthen’. The information I used is obtained from the Corpus Tracts (Transformer), where it is used to transform sequence of words into words.', 'The final objective of this article is to show how it is possible to extract a\n",
            "\n",
            "[301 | 718.20] loss=2.49 avg=2.70\n",
            "[302 | 720.46] loss=2.75 avg=2.70\n",
            "[303 | 722.71] loss=2.86 avg=2.71\n",
            "[304 | 724.95] loss=2.94 avg=2.71\n",
            "[305 | 727.19] loss=3.01 avg=2.71\n",
            "[306 | 729.44] loss=2.75 avg=2.71\n",
            "[307 | 731.68] loss=2.33 avg=2.71\n",
            "[308 | 733.93] loss=2.64 avg=2.71\n",
            "[309 | 736.18] loss=2.46 avg=2.71\n",
            "[310 | 738.43] loss=2.78 avg=2.71\n",
            "[311 | 740.68] loss=2.40 avg=2.70\n",
            "[312 | 742.93] loss=2.59 avg=2.70\n",
            "[313 | 745.17] loss=2.51 avg=2.70\n",
            "[314 | 747.42] loss=2.50 avg=2.70\n",
            "[315 | 749.67] loss=2.67 avg=2.70\n",
            "[316 | 751.92] loss=2.33 avg=2.69\n",
            "[317 | 754.17] loss=2.57 avg=2.69\n",
            "[318 | 756.41] loss=2.27 avg=2.69\n",
            "[319 | 758.66] loss=2.64 avg=2.69\n",
            "[320 | 760.90] loss=2.51 avg=2.69\n",
            "[321 | 763.15] loss=2.42 avg=2.68\n",
            "[322 | 765.39] loss=2.35 avg=2.68\n",
            "[323 | 767.64] loss=2.44 avg=2.68\n",
            "[324 | 769.89] loss=2.58 avg=2.68\n",
            "[325 | 772.14] loss=2.43 avg=2.67\n",
            "[326 | 774.38] loss=2.45 avg=2.67\n",
            "[327 | 776.62] loss=2.44 avg=2.67\n",
            "[328 | 778.87] loss=2.41 avg=2.67\n",
            "[329 | 781.11] loss=2.18 avg=2.66\n",
            "[330 | 783.36] loss=2.54 avg=2.66\n",
            "[331 | 785.61] loss=2.33 avg=2.66\n",
            "[332 | 787.86] loss=2.17 avg=2.65\n",
            "[333 | 790.10] loss=2.43 avg=2.65\n",
            "[334 | 792.35] loss=2.15 avg=2.64\n",
            "[335 | 794.59] loss=2.33 avg=2.64\n",
            "[336 | 796.85] loss=2.46 avg=2.64\n",
            "[337 | 799.10] loss=2.32 avg=2.63\n",
            "[338 | 801.34] loss=2.18 avg=2.63\n",
            "[339 | 803.59] loss=2.28 avg=2.63\n",
            "[340 | 805.84] loss=2.26 avg=2.62\n",
            "[341 | 808.09] loss=2.30 avg=2.62\n",
            "[342 | 810.33] loss=2.45 avg=2.62\n",
            "[343 | 812.57] loss=2.47 avg=2.62\n",
            "[344 | 814.82] loss=2.55 avg=2.62\n",
            "[345 | 817.07] loss=2.71 avg=2.62\n",
            "[346 | 819.31] loss=2.73 avg=2.62\n",
            "[347 | 821.56] loss=2.56 avg=2.62\n",
            "[348 | 823.81] loss=2.66 avg=2.62\n",
            "[349 | 826.06] loss=2.61 avg=2.62\n",
            "[350 | 828.30] loss=2.32 avg=2.61\n",
            "[351 | 830.56] loss=2.51 avg=2.61\n",
            "[352 | 832.80] loss=2.49 avg=2.61\n",
            "[353 | 835.05] loss=2.62 avg=2.61\n",
            "[354 | 837.30] loss=2.48 avg=2.61\n",
            "[355 | 839.54] loss=2.17 avg=2.61\n",
            "[356 | 841.80] loss=2.18 avg=2.60\n",
            "[357 | 844.05] loss=2.82 avg=2.60\n",
            "[358 | 846.30] loss=2.30 avg=2.60\n",
            "[359 | 848.55] loss=2.23 avg=2.60\n",
            "[360 | 850.80] loss=2.49 avg=2.60\n",
            "[361 | 853.04] loss=2.48 avg=2.59\n",
            "[362 | 855.30] loss=2.24 avg=2.59\n",
            "[363 | 857.55] loss=2.25 avg=2.59\n",
            "[364 | 859.79] loss=1.93 avg=2.58\n",
            "[365 | 862.04] loss=2.26 avg=2.58\n",
            "[366 | 864.29] loss=2.28 avg=2.57\n",
            "[367 | 866.54] loss=2.77 avg=2.58\n",
            "[368 | 868.79] loss=2.32 avg=2.57\n",
            "[369 | 871.04] loss=2.18 avg=2.57\n",
            "[370 | 873.28] loss=2.64 avg=2.57\n",
            "[371 | 875.53] loss=2.12 avg=2.57\n",
            "[372 | 877.78] loss=2.83 avg=2.57\n",
            "[373 | 880.03] loss=2.57 avg=2.57\n",
            "[374 | 882.28] loss=2.86 avg=2.57\n",
            "[375 | 884.53] loss=2.46 avg=2.57\n",
            "[376 | 886.78] loss=2.31 avg=2.57\n",
            "[377 | 889.03] loss=2.09 avg=2.56\n",
            "[378 | 891.28] loss=2.11 avg=2.56\n",
            "[379 | 893.53] loss=2.49 avg=2.56\n",
            "[380 | 895.79] loss=2.45 avg=2.56\n",
            "[381 | 898.04] loss=2.40 avg=2.55\n",
            "[382 | 900.29] loss=2.25 avg=2.55\n",
            "[383 | 902.54] loss=2.22 avg=2.55\n",
            "[384 | 904.79] loss=2.23 avg=2.55\n",
            "[385 | 907.03] loss=1.89 avg=2.54\n",
            "[386 | 909.28] loss=2.49 avg=2.54\n",
            "[387 | 911.52] loss=2.20 avg=2.53\n",
            "[388 | 913.77] loss=2.58 avg=2.53\n",
            "[389 | 916.02] loss=2.45 avg=2.53\n",
            "[390 | 918.26] loss=2.69 avg=2.54\n",
            "[391 | 920.51] loss=2.16 avg=2.53\n",
            "[392 | 922.76] loss=2.20 avg=2.53\n",
            "[393 | 925.00] loss=2.38 avg=2.53\n",
            "[394 | 927.25] loss=2.12 avg=2.52\n",
            "[395 | 929.50] loss=2.52 avg=2.52\n",
            "[396 | 931.75] loss=2.21 avg=2.52\n",
            "[397 | 934.00] loss=2.21 avg=2.52\n",
            "[398 | 936.25] loss=2.22 avg=2.51\n",
            "[399 | 938.50] loss=2.63 avg=2.51\n",
            "[400 | 940.75] loss=2.32 avg=2.51\n",
            "======== SAMPLE 1 ========\n",
            " ’’’’ ‘’’’’’’’’’’’’’’’ ‘’’’’ The above words have been added because it’s a bit like looking through a picture, and they’re all literal, no matter how you understand them.', 'All quotations should be taken with a grain of salt, especially if you only use the context to explain something. Use the following command to see if there are any substitutions (“‘‘’‘’’’’’’’’’’’’’’’’’’, “‘’‘’) where ‘’’’ is the phrase you’re interested in reading, and ’’’ is the verb you’re interested in understanding.', 'I’m a big fan of conversational technology, and this is perfect for building conversational applications with Siri, Alexa, etc.', 'The code follows a very basic pattern, except it uses Google’s custom built voice assistant to talk to you every time you speak. So, if you want to build an Alexa assistant, use one of these voice assistant commands:', 'This will talk to you about things like weather, rent, education, weather forecast — this should be done one time or every time you speak. There are many other ways to build this model, so please read this section if you just need to show a real-world example.', 'Let’s say your business is trying to identify the best selling products.', 'Now, you have a category search engine with a category number of Category-3000. You want to tell Siri to be more specific, the way you are doing with your main categories like text, voice or voice assistant. What this can do is create a new text query or create a dictionary with the proper data. The dictionary is created as follows.', 'I used the Google API when creating a dictionary, which you can download for free here.', 'There is an additional option called querySelectorIndices which sets a value that is passed to the QuerySelector, which returns a hash with the strings that match that query. So, for example, the string ‘cat food’ will match ‘cat food’.', 'You can also use the hash to select an item, then querySelectorIndices can retrieve the search results.', 'The above code returns the following result:', 'This will return a list of strings that are [String, String] , or dictionary strings, with [String, dictionary] pairs, or one string at a time. If you use all possible combinations of hash, search, querySelectorIndices, or hash, you should return more items in the dictionary.', 'In a few lines of code, you are ready to deploy your application using PySide-UC.', 'If you don’t see this tutorial, there is another one. To get this one, you must download and install the python3 tool, and follow these steps.', 'To get this part first, download the required packages from here. Then, open the terminal and type:', 'The file called pip3 can contain the following contents:', 'When you see a folder with your Python file, open up a command prompt and type:', 'To see a path where a file called pip3 can contain the path path and Python file path, you must type:', 'There are two file types in the file, the file and python.', 'Notice that the path is in bold, not some random dot in the prompt. Here is an example of a file open in a shell window for easy customization. You can also open the file from any computer using a command line like bash.', 'To save the path (or anything else), replace all punctuation with [\\t]'.', 'I would recommend using a file system to save files, but if you need to run commands, then use a physical folder first. If you see the error, 'Folder not enough space', then you need to open a terminal. When that happens, you are good to go.', 'To run this Python program in a virtual environment, you must type:', 'To run the PySide Python Wrapper program in a Python file, you must type the following command:', 'For this tutorial, we only need to install the python3 tool, and to use the Python program. If you run this program after installing the Python tool, or you’re not sure, then run this command and try to run it again.', 'If the problem occurs after this installation, you’ll have to do the following\n",
            "\n",
            "[401 | 954.48] loss=2.79 avg=2.52\n",
            "[402 | 956.72] loss=2.04 avg=2.51\n",
            "[403 | 958.97] loss=2.04 avg=2.51\n",
            "[404 | 961.22] loss=1.90 avg=2.50\n",
            "[405 | 963.47] loss=2.38 avg=2.50\n",
            "[406 | 965.71] loss=2.42 avg=2.50\n",
            "[407 | 967.97] loss=2.40 avg=2.50\n",
            "[408 | 970.21] loss=2.55 avg=2.50\n",
            "[409 | 972.46] loss=2.31 avg=2.50\n",
            "[410 | 974.70] loss=2.23 avg=2.49\n",
            "[411 | 976.95] loss=2.35 avg=2.49\n",
            "[412 | 979.19] loss=2.44 avg=2.49\n",
            "[413 | 981.44] loss=2.15 avg=2.49\n",
            "[414 | 983.70] loss=2.14 avg=2.48\n",
            "[415 | 985.95] loss=2.40 avg=2.48\n",
            "[416 | 988.19] loss=2.73 avg=2.49\n",
            "[417 | 990.44] loss=2.55 avg=2.49\n",
            "[418 | 992.69] loss=2.35 avg=2.48\n",
            "[419 | 994.94] loss=2.00 avg=2.48\n",
            "[420 | 997.19] loss=2.29 avg=2.48\n",
            "[421 | 999.43] loss=2.24 avg=2.48\n",
            "[422 | 1001.69] loss=2.40 avg=2.47\n",
            "[423 | 1003.93] loss=2.46 avg=2.47\n",
            "[424 | 1006.18] loss=2.38 avg=2.47\n",
            "[425 | 1008.43] loss=2.13 avg=2.47\n",
            "[426 | 1010.68] loss=2.52 avg=2.47\n",
            "[427 | 1012.93] loss=2.69 avg=2.47\n",
            "[428 | 1015.17] loss=2.10 avg=2.47\n",
            "[429 | 1017.42] loss=2.26 avg=2.47\n",
            "[430 | 1019.66] loss=2.40 avg=2.47\n",
            "[431 | 1021.91] loss=2.30 avg=2.46\n",
            "[432 | 1024.15] loss=2.22 avg=2.46\n",
            "[433 | 1026.40] loss=2.54 avg=2.46\n",
            "[434 | 1028.65] loss=1.98 avg=2.46\n",
            "[435 | 1030.89] loss=2.37 avg=2.46\n",
            "[436 | 1033.13] loss=2.04 avg=2.45\n",
            "[437 | 1035.38] loss=2.70 avg=2.46\n",
            "[438 | 1037.63] loss=2.27 avg=2.45\n",
            "[439 | 1039.88] loss=2.60 avg=2.45\n",
            "[440 | 1042.13] loss=2.21 avg=2.45\n",
            "[441 | 1044.38] loss=2.16 avg=2.45\n",
            "[442 | 1046.63] loss=2.30 avg=2.45\n",
            "[443 | 1048.87] loss=2.13 avg=2.44\n",
            "[444 | 1051.13] loss=2.24 avg=2.44\n",
            "[445 | 1053.37] loss=2.33 avg=2.44\n",
            "[446 | 1055.62] loss=2.27 avg=2.44\n",
            "[447 | 1057.87] loss=2.84 avg=2.44\n",
            "[448 | 1060.12] loss=2.12 avg=2.44\n",
            "[449 | 1062.37] loss=2.10 avg=2.44\n",
            "[450 | 1064.62] loss=2.46 avg=2.44\n",
            "[451 | 1066.87] loss=2.22 avg=2.44\n",
            "[452 | 1069.11] loss=1.98 avg=2.43\n",
            "[453 | 1071.36] loss=2.43 avg=2.43\n",
            "[454 | 1073.61] loss=2.14 avg=2.43\n",
            "[455 | 1075.85] loss=2.09 avg=2.42\n",
            "[456 | 1078.10] loss=2.17 avg=2.42\n",
            "[457 | 1080.35] loss=2.37 avg=2.42\n",
            "[458 | 1082.59] loss=2.02 avg=2.42\n",
            "[459 | 1084.83] loss=2.15 avg=2.41\n",
            "[460 | 1087.08] loss=2.24 avg=2.41\n",
            "[461 | 1089.33] loss=1.85 avg=2.41\n",
            "[462 | 1091.57] loss=1.94 avg=2.40\n",
            "[463 | 1093.82] loss=2.46 avg=2.40\n",
            "[464 | 1096.07] loss=2.25 avg=2.40\n",
            "[465 | 1098.31] loss=2.20 avg=2.40\n",
            "[466 | 1100.56] loss=1.91 avg=2.39\n",
            "[467 | 1102.81] loss=1.90 avg=2.39\n",
            "[468 | 1105.07] loss=2.33 avg=2.39\n",
            "[469 | 1107.31] loss=2.24 avg=2.39\n",
            "[470 | 1109.56] loss=2.14 avg=2.38\n",
            "[471 | 1111.81] loss=2.39 avg=2.38\n",
            "[472 | 1114.05] loss=1.98 avg=2.38\n",
            "[473 | 1116.29] loss=2.28 avg=2.38\n",
            "[474 | 1118.54] loss=1.88 avg=2.37\n",
            "[475 | 1120.79] loss=2.14 avg=2.37\n",
            "[476 | 1123.03] loss=2.10 avg=2.37\n",
            "[477 | 1125.28] loss=2.17 avg=2.37\n",
            "[478 | 1127.52] loss=2.11 avg=2.36\n",
            "[479 | 1129.77] loss=2.47 avg=2.37\n",
            "[480 | 1132.02] loss=2.60 avg=2.37\n",
            "[481 | 1134.26] loss=2.42 avg=2.37\n",
            "[482 | 1136.51] loss=1.96 avg=2.36\n",
            "[483 | 1138.76] loss=1.69 avg=2.36\n",
            "[484 | 1141.00] loss=2.42 avg=2.36\n",
            "[485 | 1143.25] loss=1.92 avg=2.35\n",
            "[486 | 1145.50] loss=1.88 avg=2.35\n",
            "[487 | 1147.74] loss=2.17 avg=2.35\n",
            "[488 | 1149.99] loss=2.25 avg=2.35\n",
            "[489 | 1152.24] loss=2.51 avg=2.35\n",
            "[490 | 1154.49] loss=1.62 avg=2.34\n",
            "[491 | 1156.74] loss=1.60 avg=2.33\n",
            "[492 | 1158.98] loss=2.23 avg=2.33\n",
            "[493 | 1161.23] loss=1.84 avg=2.33\n",
            "[494 | 1163.48] loss=1.87 avg=2.32\n",
            "[495 | 1165.72] loss=2.11 avg=2.32\n",
            "[496 | 1167.97] loss=1.63 avg=2.31\n",
            "[497 | 1170.21] loss=1.84 avg=2.31\n",
            "[498 | 1172.46] loss=2.19 avg=2.31\n",
            "[499 | 1174.70] loss=2.24 avg=2.31\n",
            "[500 | 1176.95] loss=1.86 avg=2.30\n",
            "======== SAMPLE 1 ========\n",
            ".' For training, we use a combination of transformer models to generate training and inference data from the input data (generating text vector values for each word and using that values as feed) and an internal discriminator to classify the training data into multiple classes called a hidden classifier.', 'The transformer model takes a word on each word dictionary, generates words by itself from the dictionary, then looks through each word by its corresponding word vector in a hidden classifier dictionary. Then in a bidirectional fashion, the model generates word embeddings using the dictionary of the words from each hidden class in the hidden class, and vice versa using the learned embeddings from those embeddings. These hidden embeddings are then passed to the feed forward network. This model is efficient and provides training results in a few hours.', 'The feed forward network is trained on all the tokens and learns on a single token. Once it has achieved its aim, it trains multiple times to check if all the tokens reached its target. This is the basis for the “fit” method. At each training step, it calculates predictions based on how fast the model was able to identify the token as the token that reached its target.', 'There are different input methods being considered like token selection, label fetching and many more.', 'For all input methods, the model will predict the predicted token based on how well the model guessed it. These prediction methods will be initialized with the input text. This tells the generator the number of predictors (if predicted), if it already predicted the predicted token, and the number of input sentences it would take to train the model. The output text for each prediction method depends on the number of hidden classes in the model.', 'To get a better idea of the types of input methods, one can check out the tf-idf file in the Google library folder in the root directory (make sure you have at least one of your libraries/idf files — it usually contains them).', 'The tf-idf file contains various information about the generated text. The simplest way is to load one of the generated text files and use one of the generated text files into the model.', 'To train the model further, we need to take the words inside a tag. Words are passed inside of a hidden class and their embeddings will be transformed to embeddings for the unseen class. In the above example, we pass an unseen class into a hidden class and a tag generator. Then, in a bidirectional fashion, the model would predict the masked word vectors for the word vectors in the hidden class (by passing the mask tag to the model).', 'We can get more details on the tf-idf file here: https://github.com/google/transformers/blob/2a42d5c9b9', 'Let’s jump right into it!', 'It is a deep learning method used for getting the text representation of words inside a text. Similar models can be used to do text summarization, but this time, to have more information to improve learning.', 'The main purpose of this model is to pick the best sentence for text summarization. Before we get started let me just show some examples. Let’s create some boxes in the sentence —', 'There are 3 types of boxes that we will be creating. There are two different type of boxes that we will have to get “sentiment” and “regression” (the models will be able to find the ‘regressive’)', '1st type of boxes: “correlation, “relation-or-bleeding”', '2nd type of boxes: “convallment, “convallment, contagion”', 'If you’d like to check it out!', 'Train the model each sentence like this.', 'Then in train mode, input to the model, train_split, goes through its text segments (separate segments for each sentence). Then use that input to get the maximum similarity score for the input. Remember the last thing? The last thing that happens after you input two sentences of the same token in train mode? You get that last sentence, at loss.', 'Now we will add these 3 types of boxes into the model.', 'Note: We also get the score using the “mean” and “predict” threshold that were calculated earlier to filter out outliers (the ones with higher scores). Remember the last thing that happens? The last thing that happens after we get the score for the input input? If we get the score from the discriminator we will be doing the ranking and then we will use it for the model. In that way, we have a better way to filter out more tokens than the mean score.', 'The discriminator that will be used is a pre-trained algorithm or an automated classifier.\n",
            "\n",
            "[501 | 1190.73] loss=2.16 avg=2.30\n",
            "[502 | 1192.98] loss=1.91 avg=2.30\n",
            "[503 | 1195.22] loss=2.09 avg=2.30\n",
            "[504 | 1197.46] loss=1.98 avg=2.29\n",
            "[505 | 1199.71] loss=2.05 avg=2.29\n",
            "[506 | 1201.96] loss=2.20 avg=2.29\n",
            "[507 | 1204.20] loss=2.32 avg=2.29\n",
            "[508 | 1206.45] loss=2.28 avg=2.29\n",
            "[509 | 1208.70] loss=2.74 avg=2.29\n",
            "[510 | 1210.95] loss=1.68 avg=2.29\n",
            "[511 | 1213.19] loss=1.80 avg=2.28\n",
            "[512 | 1215.44] loss=1.96 avg=2.28\n",
            "[513 | 1217.68] loss=2.22 avg=2.28\n",
            "[514 | 1219.92] loss=1.93 avg=2.27\n",
            "[515 | 1222.17] loss=1.79 avg=2.27\n",
            "[516 | 1224.42] loss=1.83 avg=2.27\n",
            "[517 | 1226.66] loss=2.01 avg=2.26\n",
            "[518 | 1228.92] loss=1.89 avg=2.26\n",
            "[519 | 1231.17] loss=2.23 avg=2.26\n",
            "[520 | 1233.41] loss=2.35 avg=2.26\n",
            "[521 | 1235.66] loss=2.00 avg=2.26\n",
            "[522 | 1237.91] loss=1.94 avg=2.25\n",
            "[523 | 1240.15] loss=2.21 avg=2.25\n",
            "[524 | 1242.40] loss=1.88 avg=2.25\n",
            "[525 | 1244.65] loss=1.87 avg=2.25\n",
            "[526 | 1246.89] loss=1.87 avg=2.24\n",
            "[527 | 1249.14] loss=2.08 avg=2.24\n",
            "[528 | 1251.39] loss=1.79 avg=2.24\n",
            "[529 | 1253.63] loss=2.03 avg=2.23\n",
            "[530 | 1255.87] loss=2.20 avg=2.23\n",
            "[531 | 1258.11] loss=2.44 avg=2.24\n",
            "[532 | 1260.36] loss=1.94 avg=2.23\n",
            "[533 | 1262.60] loss=2.10 avg=2.23\n",
            "[534 | 1264.85] loss=2.41 avg=2.23\n",
            "[535 | 1267.08] loss=1.99 avg=2.23\n",
            "[536 | 1269.33] loss=1.98 avg=2.23\n",
            "[537 | 1271.58] loss=1.91 avg=2.22\n",
            "[538 | 1273.82] loss=2.34 avg=2.23\n",
            "[539 | 1276.07] loss=2.19 avg=2.23\n",
            "[540 | 1278.32] loss=2.11 avg=2.22\n",
            "[541 | 1280.57] loss=1.99 avg=2.22\n",
            "[542 | 1282.82] loss=1.77 avg=2.22\n",
            "[543 | 1285.07] loss=1.77 avg=2.21\n",
            "[544 | 1287.31] loss=2.14 avg=2.21\n",
            "[545 | 1289.56] loss=2.03 avg=2.21\n",
            "[546 | 1291.81] loss=1.93 avg=2.21\n",
            "[547 | 1294.06] loss=2.36 avg=2.21\n",
            "[548 | 1296.30] loss=1.68 avg=2.20\n",
            "[549 | 1298.55] loss=1.71 avg=2.20\n",
            "[550 | 1300.80] loss=2.01 avg=2.20\n",
            "[551 | 1303.05] loss=2.20 avg=2.20\n",
            "[552 | 1305.30] loss=2.09 avg=2.20\n",
            "[553 | 1307.54] loss=2.00 avg=2.19\n",
            "[554 | 1309.78] loss=2.05 avg=2.19\n",
            "[555 | 1312.03] loss=2.16 avg=2.19\n",
            "[556 | 1314.28] loss=2.00 avg=2.19\n",
            "[557 | 1316.53] loss=2.04 avg=2.19\n",
            "[558 | 1318.79] loss=1.84 avg=2.19\n",
            "[559 | 1321.03] loss=2.23 avg=2.19\n",
            "[560 | 1323.27] loss=1.90 avg=2.18\n",
            "[561 | 1325.52] loss=2.16 avg=2.18\n",
            "[562 | 1327.76] loss=1.83 avg=2.18\n",
            "[563 | 1330.00] loss=2.11 avg=2.18\n",
            "[564 | 1332.25] loss=1.91 avg=2.18\n",
            "[565 | 1334.50] loss=2.07 avg=2.17\n",
            "[566 | 1336.74] loss=2.24 avg=2.18\n",
            "[567 | 1338.98] loss=1.66 avg=2.17\n",
            "[568 | 1341.23] loss=2.24 avg=2.17\n",
            "[569 | 1343.47] loss=2.33 avg=2.17\n",
            "[570 | 1345.72] loss=1.79 avg=2.17\n",
            "[571 | 1347.97] loss=1.78 avg=2.16\n",
            "[572 | 1350.21] loss=1.82 avg=2.16\n",
            "[573 | 1352.47] loss=2.04 avg=2.16\n",
            "[574 | 1354.72] loss=2.17 avg=2.16\n",
            "[575 | 1356.97] loss=1.28 avg=2.15\n",
            "[576 | 1359.21] loss=1.85 avg=2.15\n",
            "[577 | 1361.46] loss=2.03 avg=2.15\n",
            "[578 | 1363.71] loss=1.77 avg=2.14\n",
            "[579 | 1365.95] loss=1.88 avg=2.14\n",
            "[580 | 1368.20] loss=1.76 avg=2.14\n",
            "[581 | 1370.46] loss=1.60 avg=2.13\n",
            "[582 | 1372.70] loss=1.78 avg=2.13\n",
            "[583 | 1374.96] loss=1.57 avg=2.12\n",
            "[584 | 1377.21] loss=1.94 avg=2.12\n",
            "[585 | 1379.46] loss=1.92 avg=2.12\n",
            "[586 | 1381.70] loss=1.69 avg=2.11\n",
            "[587 | 1383.95] loss=1.64 avg=2.11\n",
            "[588 | 1386.20] loss=1.80 avg=2.11\n",
            "[589 | 1388.44] loss=2.17 avg=2.11\n",
            "[590 | 1390.69] loss=1.46 avg=2.10\n",
            "[591 | 1392.94] loss=2.01 avg=2.10\n",
            "[592 | 1395.19] loss=1.64 avg=2.10\n",
            "[593 | 1397.43] loss=2.06 avg=2.09\n",
            "[594 | 1399.68] loss=1.83 avg=2.09\n",
            "[595 | 1401.93] loss=1.89 avg=2.09\n",
            "[596 | 1404.17] loss=1.78 avg=2.09\n",
            "[597 | 1406.42] loss=2.23 avg=2.09\n",
            "[598 | 1408.67] loss=2.02 avg=2.09\n",
            "[599 | 1410.92] loss=1.77 avg=2.08\n",
            "[600 | 1413.17] loss=1.91 avg=2.08\n",
            "======== SAMPLE 1 ========\n",
            ": “You forgot your jacket, how you’re back at school?”. If we take ‘in’ the jacket’s absence from this question we get: Where did you get it’s?', 'While this question may be answered with a ‘a’ answer, a ‘top down’ answer would go beyond that. The answer would lie in a series of hidden sub questions that have no meaningful meaning and were simply asked to help answer this question.', 'A TOP-SEQUENCE answer is one that can be determined with some sophisticated NLP software.', 'The question is: Given an “output” of a text. “should be a text-based learner” method? For example, if the output of an IMDb class goes to “How many times do you die young?” we want the answer to be somewhere in the range of 1–10. We also want an entire movie that goes through those deaths to “explain.” Note: “How many times do you die young?” is a very narrow range, but is obviously true when asked to give a ‘10’ answer. As such, if this answer were a text method this one would be at the top of this list, as it would tell us, in a meaningful way, what an IMDb class might have looked like.', 'An “top down” approach would, however, lie in the finding of the ‘top up’ method—that is, where an output of an input text comes directly into the front-end of a class class.', 'A TOP-SEQUENCE model would, in the sense of having an end-to-end view of a text-based text, only include these end-to-end view, and so, in theory, this model would not need to be exposed. However, one would have to place the class model on top of the class and build the “visual learner” view, to see how this could work.', 'To do this, the class would begin to focus on the pre-built texts in the class for the question and pass it through the ‘view’ method, as seen below: For each pre-built text, the class would focus on the text from the ‘top down’ class as it passes through the class class, in a non-linear manner.', 'The class itself would then focus on the pre-built texts in the class, in sequential order, and, finally, it would focus on the class at the top of the class, where it would see the pre-built texts from that text.', 'This sequential-looking view of texts from the pre-built texts, in turn, would look back into the main model of this class. There, in this sequential view, it would find these texts and attempt to predict the ‘top up’ method.', 'This is where the Visual Basic Classifier came into play. It provides pre-built text as inputs to the class, and then, in a bid to reproduce the same method, employs these pre-built texts in order to generate predictions.', 'The entire process, in which the class and the class-output are equivalent, can be further illustrated using a simplified version of a Biomarse Deep Learning Architecture with a LDA with Masked Convolutional Neural Networks:', 'The complete architecture, with the text and the class both in two big pallets, can be seen below:', 'As this diagram shows, the classifier sees only texts that have been explicitly supervised.', 'This further illustrates how very good the final model of this architecture was, and of the many errors, incomplete predictions are left unpunished.', 'At the heart of this architecture, though, this was built to tackle “not guilty” statements: That you ate a banana, or That your wife’s boyfriend tried to rob you of your virginity.', 'This further illustrates how very good the final model of this architecture was, and of the many errors, incomplete predictions were left unpunished.', 'The complete architecture, with the text and the class both in two big pallets, can be seen below:', 'This further illustrates how very good the final model of this architecture was, and of the many errors, incomplete predictions were left unpunished.', 'The loss-per-consequence model uses an algorithm that is very simple to implement: If the input text is shorter than the target length, the model goes into action. The shorter the input text is, the higher the output text.', 'For our example, the loss per paragraph is the loss (in terms of word usage) for an average reader of a news article. So the higher the loss (to transaction cost) for a user, the higher the loss (to honest expression) it is.'\n",
            "\n",
            "[601 | 1426.70] loss=2.03 avg=2.08\n",
            "[602 | 1428.94] loss=1.70 avg=2.08\n",
            "[603 | 1431.19] loss=1.84 avg=2.08\n",
            "[604 | 1433.44] loss=1.90 avg=2.07\n",
            "[605 | 1435.68] loss=2.14 avg=2.07\n",
            "[606 | 1437.92] loss=1.85 avg=2.07\n",
            "[607 | 1440.17] loss=1.76 avg=2.07\n",
            "[608 | 1442.41] loss=1.71 avg=2.07\n",
            "[609 | 1444.65] loss=1.71 avg=2.06\n",
            "[610 | 1446.89] loss=2.15 avg=2.06\n",
            "[611 | 1449.13] loss=2.08 avg=2.06\n",
            "[612 | 1451.37] loss=1.76 avg=2.06\n",
            "[613 | 1453.61] loss=1.68 avg=2.06\n",
            "[614 | 1455.85] loss=1.62 avg=2.05\n",
            "[615 | 1458.09] loss=1.91 avg=2.05\n",
            "[616 | 1460.34] loss=1.81 avg=2.05\n",
            "[617 | 1462.59] loss=1.92 avg=2.05\n",
            "[618 | 1464.84] loss=1.19 avg=2.04\n",
            "[619 | 1467.08] loss=1.75 avg=2.04\n",
            "[620 | 1469.32] loss=1.78 avg=2.03\n",
            "[621 | 1471.56] loss=1.77 avg=2.03\n",
            "[622 | 1473.80] loss=2.19 avg=2.03\n",
            "[623 | 1476.04] loss=2.07 avg=2.03\n",
            "[624 | 1478.29] loss=1.94 avg=2.03\n",
            "[625 | 1480.54] loss=1.62 avg=2.03\n",
            "[626 | 1482.79] loss=1.73 avg=2.02\n",
            "[627 | 1485.03] loss=1.77 avg=2.02\n",
            "[628 | 1487.28] loss=1.68 avg=2.02\n",
            "[629 | 1489.52] loss=1.91 avg=2.02\n",
            "[630 | 1491.77] loss=1.83 avg=2.02\n",
            "[631 | 1494.01] loss=1.54 avg=2.01\n",
            "[632 | 1496.26] loss=2.08 avg=2.01\n",
            "[633 | 1498.51] loss=1.41 avg=2.01\n",
            "[634 | 1500.76] loss=2.16 avg=2.01\n",
            "[635 | 1503.00] loss=1.98 avg=2.01\n",
            "[636 | 1505.25] loss=1.68 avg=2.00\n",
            "[637 | 1507.50] loss=1.57 avg=2.00\n",
            "[638 | 1509.75] loss=1.57 avg=1.99\n",
            "[639 | 1512.00] loss=1.72 avg=1.99\n",
            "[640 | 1514.25] loss=2.04 avg=1.99\n",
            "[641 | 1516.50] loss=2.25 avg=2.00\n",
            "[642 | 1518.75] loss=1.58 avg=1.99\n",
            "[643 | 1521.00] loss=2.02 avg=1.99\n",
            "[644 | 1523.25] loss=1.84 avg=1.99\n",
            "[645 | 1525.50] loss=1.73 avg=1.99\n",
            "[646 | 1527.74] loss=1.71 avg=1.98\n",
            "[647 | 1529.98] loss=2.03 avg=1.98\n",
            "[648 | 1532.23] loss=1.60 avg=1.98\n",
            "[649 | 1534.47] loss=1.74 avg=1.98\n",
            "[650 | 1536.72] loss=1.94 avg=1.98\n",
            "[651 | 1538.97] loss=1.52 avg=1.97\n",
            "[652 | 1541.22] loss=1.89 avg=1.97\n",
            "[653 | 1543.47] loss=1.87 avg=1.97\n",
            "[654 | 1545.72] loss=1.53 avg=1.97\n",
            "[655 | 1547.97] loss=1.89 avg=1.97\n",
            "[656 | 1550.22] loss=1.80 avg=1.96\n",
            "[657 | 1552.47] loss=2.08 avg=1.97\n",
            "[658 | 1554.72] loss=1.72 avg=1.96\n",
            "[659 | 1556.97] loss=1.97 avg=1.96\n",
            "[660 | 1559.22] loss=1.73 avg=1.96\n",
            "[661 | 1561.47] loss=1.43 avg=1.96\n",
            "[662 | 1563.72] loss=1.68 avg=1.95\n",
            "[663 | 1565.96] loss=1.96 avg=1.95\n",
            "[664 | 1568.22] loss=1.59 avg=1.95\n",
            "[665 | 1570.46] loss=1.76 avg=1.95\n",
            "[666 | 1572.71] loss=1.88 avg=1.95\n",
            "[667 | 1574.96] loss=1.97 avg=1.95\n",
            "[668 | 1577.21] loss=2.15 avg=1.95\n",
            "[669 | 1579.46] loss=1.62 avg=1.95\n",
            "[670 | 1581.70] loss=1.90 avg=1.95\n",
            "[671 | 1583.96] loss=1.63 avg=1.94\n",
            "[672 | 1586.21] loss=1.91 avg=1.94\n",
            "[673 | 1588.45] loss=1.60 avg=1.94\n",
            "[674 | 1590.71] loss=1.27 avg=1.93\n",
            "[675 | 1592.95] loss=1.66 avg=1.93\n",
            "[676 | 1595.20] loss=1.77 avg=1.93\n",
            "[677 | 1597.45] loss=1.66 avg=1.92\n",
            "[678 | 1599.70] loss=1.57 avg=1.92\n",
            "[679 | 1601.95] loss=1.57 avg=1.92\n",
            "[680 | 1604.19] loss=1.70 avg=1.92\n",
            "[681 | 1606.45] loss=1.97 avg=1.92\n",
            "[682 | 1608.69] loss=1.63 avg=1.91\n",
            "[683 | 1610.94] loss=1.87 avg=1.91\n",
            "[684 | 1613.19] loss=1.47 avg=1.91\n",
            "[685 | 1615.44] loss=1.51 avg=1.90\n",
            "[686 | 1617.69] loss=2.06 avg=1.91\n",
            "[687 | 1619.94] loss=1.67 avg=1.90\n",
            "[688 | 1622.19] loss=1.93 avg=1.90\n",
            "[689 | 1624.44] loss=1.43 avg=1.90\n",
            "[690 | 1626.69] loss=2.01 avg=1.90\n",
            "[691 | 1628.94] loss=1.65 avg=1.90\n",
            "[692 | 1631.18] loss=2.23 avg=1.90\n",
            "[693 | 1633.43] loss=1.92 avg=1.90\n",
            "[694 | 1635.68] loss=1.69 avg=1.90\n",
            "[695 | 1637.92] loss=1.81 avg=1.90\n",
            "[696 | 1640.17] loss=1.61 avg=1.90\n",
            "[697 | 1642.42] loss=1.54 avg=1.89\n",
            "[698 | 1644.67] loss=1.53 avg=1.89\n",
            "[699 | 1646.92] loss=1.61 avg=1.89\n",
            "[700 | 1649.17] loss=1.78 avg=1.88\n",
            "======== SAMPLE 1 ========\n",
            ", which is very much like a COVID-19 controlled pandemic. We will address the shortcomings of that approach here but for the sake of the discussion aim to build the best model available as soon as that is possible. In this article, we won’t be building a model for a pandemic-controlled dataset',, 'Let’s just sum up what we’ve learned in the article:', 'With just a couple of lines of code (one of which was uploaded here ), we generated multiple models which were then easily combined with the vanilla Sentiment analysis model to create a fully-featured and efficient tool.', 'Here’s what we’ve learned:', 'Let’s try some basic sentiment classification with these models:', 'and compare the performance:', 'The model we built below performs very similarly to our example — everything is well-defined, the sentiment classification is correct, and the text description is understandable and brief. What this means is that we are still using what we had at the start — only using the vanilla model for now.', 'We also have a better model for classification accuracy in general, OpenAI SentenceRecORD-Transformer. To get a more detailed review, you can go to here.', 'The best part of this all-or-nothing approach? We only have to set a few things constant:', 'Again, there are a few models in the SVD group (one of them was trained using the cosine similarity matrix and the other two used the HLM). We still have to get the dataset (and the necessary data) for the SVD group — we will probably have to do both — and we will need both the text description and the full text in order to really be confident that we can group together the texts used for this analysis.', 'To sum up — we built a powerful tool for sentiment analysis that outperformed only a few other tools out there. We hope that with this success, other sentiment analysis models will follow. We would also be remiss not congratulate BERT on placing second behind only highly similar BERT & Co.', 'All the code, and the data — go here if you want to train your own.', 'In the mean time, we’re going to continue on to see how SVD works!', \"A newsletter that delivers The Startup's most popular stories to your inbox once a month.\\xa0Learn more\", 'Create a free Medium account to get Top Stories in your inbox.', 'Written by', 'Written by'] ['I recently completed my PhD research in machine learning and machine learning is back. Machine learning is the science behind how machines can better understand us, our needs and influence us. Machine learning is a way computers learn and predict the best of us. Machine learning algorithms simulate real-world data while helping us achieve our daily routines through our bodies and minds.', 'Most of us are used to relying on external resources and products to do the answering for tasks, like washing dishes, answering to emails, picking up boxes for our hard drives and so on. We rely on self-service computing, automatic text and voice assistant, and text messaging apps to answer for and find answers to our every need, whether it’s our alarm clock ticking, filling the fridge, filling the fridge's, filling the fridge’s, paying for car insurance or saving the bank account. But while technology is improving, it also comes at a high cost. And that’s a problem that must be solved in a fair and just technology economy.', 'Today I want to share the real cost of self-service computing, self-service computing which is a popular name for a technology in AI. For readers who are unfamiliar with it, I’ve introduced the MSRP of £149. This includes the training and development cost, as well as data processing and support.', 'In this article, we’ll demonstrate that the costs for self-service computing are relatively reasonable compared to the competition.', 'Introduction to machine learning and machine learning is a field where computers work on computer programs to solve a particular problem. The data, the algorithms and the processing time are quite different, so it is not that different between machine learning and machine learning. For example, text summarization in text summarization is not a different problem than text text generation where the algorithm is different from the way text is represented in the book.', 'Text-to-speech technology is a computer-based form of speech recognition. It helps with recognition, too, but it isn’t enough to solve all the problems described above. Speech recognition models are trained and used to improve computer-aided design, in particular, transfer learning.', 'We will be using Natural Language API to retrieve the extracted text using natural-language-api to learn word embeddings and then using that to train and apply transfer learning on our large dataset of texts.', 'Let’s\n",
            "\n",
            "[701 | 1662.66] loss=1.75 avg=1.88\n",
            "[702 | 1664.90] loss=1.85 avg=1.88\n",
            "[703 | 1667.14] loss=1.89 avg=1.88\n",
            "[704 | 1669.39] loss=1.84 avg=1.88\n",
            "[705 | 1671.64] loss=1.85 avg=1.88\n",
            "[706 | 1673.89] loss=1.90 avg=1.88\n",
            "[707 | 1676.13] loss=1.82 avg=1.88\n",
            "[708 | 1678.38] loss=1.97 avg=1.88\n",
            "[709 | 1680.62] loss=1.53 avg=1.88\n",
            "[710 | 1682.87] loss=1.29 avg=1.87\n",
            "[711 | 1685.12] loss=1.75 avg=1.87\n",
            "[712 | 1687.37] loss=1.52 avg=1.87\n",
            "[713 | 1689.61] loss=1.70 avg=1.87\n",
            "[714 | 1691.86] loss=1.68 avg=1.86\n",
            "[715 | 1694.10] loss=1.80 avg=1.86\n",
            "[716 | 1696.34] loss=1.73 avg=1.86\n",
            "[717 | 1698.58] loss=1.42 avg=1.86\n",
            "[718 | 1700.83] loss=1.53 avg=1.86\n",
            "[719 | 1703.07] loss=1.73 avg=1.85\n",
            "[720 | 1705.31] loss=1.53 avg=1.85\n",
            "[721 | 1707.56] loss=1.91 avg=1.85\n",
            "[722 | 1709.80] loss=1.79 avg=1.85\n",
            "[723 | 1712.04] loss=1.46 avg=1.85\n",
            "[724 | 1714.29] loss=1.78 avg=1.85\n",
            "[725 | 1716.53] loss=2.04 avg=1.85\n",
            "[726 | 1718.77] loss=1.54 avg=1.84\n",
            "[727 | 1721.01] loss=1.38 avg=1.84\n",
            "[728 | 1723.26] loss=1.47 avg=1.84\n",
            "[729 | 1725.50] loss=1.53 avg=1.83\n",
            "[730 | 1727.75] loss=1.59 avg=1.83\n",
            "[731 | 1730.00] loss=1.57 avg=1.83\n",
            "[732 | 1732.25] loss=1.52 avg=1.83\n",
            "[733 | 1734.50] loss=1.31 avg=1.82\n",
            "[734 | 1736.74] loss=1.72 avg=1.82\n",
            "[735 | 1738.99] loss=1.31 avg=1.81\n",
            "[736 | 1741.24] loss=1.45 avg=1.81\n",
            "[737 | 1743.49] loss=1.39 avg=1.81\n",
            "[738 | 1745.74] loss=1.33 avg=1.80\n",
            "[739 | 1748.00] loss=1.13 avg=1.79\n",
            "[740 | 1750.25] loss=1.48 avg=1.79\n",
            "[741 | 1752.51] loss=1.49 avg=1.79\n",
            "[742 | 1754.77] loss=1.55 avg=1.79\n",
            "[743 | 1757.02] loss=1.35 avg=1.78\n",
            "[744 | 1759.27] loss=1.76 avg=1.78\n",
            "[745 | 1761.52] loss=1.51 avg=1.78\n",
            "[746 | 1763.77] loss=1.36 avg=1.77\n",
            "[747 | 1766.01] loss=1.36 avg=1.77\n",
            "[748 | 1768.27] loss=1.34 avg=1.77\n",
            "[749 | 1770.52] loss=1.77 avg=1.77\n",
            "[750 | 1772.77] loss=1.40 avg=1.76\n",
            "[751 | 1775.02] loss=1.29 avg=1.76\n",
            "[752 | 1777.27] loss=1.70 avg=1.76\n",
            "[753 | 1779.51] loss=1.42 avg=1.75\n",
            "[754 | 1781.77] loss=1.15 avg=1.75\n",
            "[755 | 1784.01] loss=1.53 avg=1.75\n",
            "[756 | 1786.26] loss=1.45 avg=1.74\n",
            "[757 | 1788.51] loss=1.23 avg=1.74\n",
            "[758 | 1790.77] loss=1.65 avg=1.74\n",
            "[759 | 1793.01] loss=1.92 avg=1.74\n",
            "[760 | 1795.26] loss=1.85 avg=1.74\n",
            "[761 | 1797.51] loss=1.77 avg=1.74\n",
            "[762 | 1799.76] loss=1.76 avg=1.74\n",
            "[763 | 1802.01] loss=1.39 avg=1.74\n",
            "[764 | 1804.25] loss=1.30 avg=1.73\n",
            "[765 | 1806.50] loss=1.61 avg=1.73\n",
            "[766 | 1808.75] loss=1.66 avg=1.73\n",
            "[767 | 1811.00] loss=1.49 avg=1.73\n",
            "[768 | 1813.24] loss=1.79 avg=1.73\n",
            "[769 | 1815.49] loss=1.27 avg=1.72\n",
            "[770 | 1817.73] loss=1.68 avg=1.72\n",
            "[771 | 1819.98] loss=1.58 avg=1.72\n",
            "[772 | 1822.22] loss=1.56 avg=1.72\n",
            "[773 | 1824.46] loss=1.41 avg=1.72\n",
            "[774 | 1826.71] loss=1.66 avg=1.72\n",
            "[775 | 1828.95] loss=1.25 avg=1.71\n",
            "[776 | 1831.20] loss=1.23 avg=1.71\n",
            "[777 | 1833.45] loss=1.31 avg=1.70\n",
            "[778 | 1835.69] loss=1.58 avg=1.70\n",
            "[779 | 1837.93] loss=1.49 avg=1.70\n",
            "[780 | 1840.18] loss=1.84 avg=1.70\n",
            "[781 | 1842.43] loss=1.80 avg=1.70\n",
            "[782 | 1844.68] loss=1.44 avg=1.70\n",
            "[783 | 1846.92] loss=1.05 avg=1.69\n",
            "[784 | 1849.17] loss=1.21 avg=1.69\n",
            "[785 | 1851.42] loss=1.33 avg=1.68\n",
            "[786 | 1853.66] loss=1.44 avg=1.68\n",
            "[787 | 1855.91] loss=1.43 avg=1.68\n",
            "[788 | 1858.15] loss=0.93 avg=1.67\n",
            "[789 | 1860.41] loss=1.51 avg=1.67\n",
            "[790 | 1862.65] loss=1.51 avg=1.67\n",
            "[791 | 1864.90] loss=1.67 avg=1.67\n",
            "[792 | 1867.15] loss=1.19 avg=1.66\n",
            "[793 | 1869.40] loss=1.74 avg=1.67\n",
            "[794 | 1871.64] loss=1.26 avg=1.66\n",
            "[795 | 1873.89] loss=1.62 avg=1.66\n",
            "[796 | 1876.14] loss=1.64 avg=1.66\n",
            "[797 | 1878.39] loss=1.10 avg=1.65\n",
            "[798 | 1880.63] loss=1.12 avg=1.65\n",
            "[799 | 1882.89] loss=1.60 avg=1.65\n",
            "[800 | 1885.14] loss=1.36 avg=1.65\n",
            "======== SAMPLE 1 ========\n",
            " in preprocessing with a preprocessing approach that is “specific” (not “specific” in the traditional sense).', 'In natural language processing (NLP), we want the sentence to capture patterns, relationships, and contexts in the text. We do not want words to make the same sense only because the structure of sentences may lead them to do some things differently. Adopting a technique with a technique that is “specific” makes the text more “relevant”.', 'Preprocessing is the process of labeling or describing the data in the data. It may seem like a huge leap from now on, when we were talking about converting text into machine-readable form but actually this is the groundwork that has been broken for a whole new generation of text processing problems. Even though we will never be able to convert a text file into machine-friendly form, we can at least try to make it less difficult for the computer to read the data. That way, the model that was developed back when on its core technology is no longer dependent on core functions and remains independent. This independence allows a preprocessing pipeline to easily harness the power of the machine.', 'In preprocessing, the preprocessing approach is based on two key steps. First, the model is first preprocessed to convert a text file into a specific mathematical definition’s definition will.tlf into “very specific”. Second, the model is first label with respect to the entire data set to determine “very specific”. By placing these labels first, preprocessing the entire set will allow the computer to readily recognize and quantify the “pattern” of the text data, which can then be used in identifying patterns in the data in the same way as a human would recognize a handwriting line in a written text.', 'The preprocessing method has certain drawbacks to preprocessing data in this format. The most important negatives count in preprocessing are the ratio of the count of the negative counts multiplied by the negatives divided by the count of the number of negatives, which generally ranges from 0 to 1. As a result, the preprocessing steps tend to add complexity to the definition of the data quite a bit.', 'Below are some data examples taken from the paper:', 'When the preprocessing of the data is done, the preprocessing approach used for the original data is a combination of the two methods. The first one is a simplified version of the original data and the last one is a more complex preprocessing approach used to convert the original data into machine-friendly form. The authors have used these approaches to separate the data on both the left and the right where it is possible to separate the data on both the left and the right.', 'Preprocessing is done in two separate steps. The first one is a simplified version of the original data and the second one is a more complex preprocessing approach used to convert the data into machine-friendly form. The first procedure is based on counting negatives and calculating the ratio of the count of the negative counts versus the count of the number of negatives in the data. The calculation takes into account the relationship between the count of the negatives and the count of the number of negatives in the data. The count of the negatives is also calculated separately for each part of the data. Here, ‘positive’ and ‘negative’ are counted in the same way. Then, the second part of the procedure is based on the counting of negatives with respect to parts of the original data only. There are different ways of computing the ratio of the count of the negative counts versus the count of the number of negatives in the data. For example, the ratio of the count of the negatives with respect to parts of the original data like air and popcorn can be calculated using this formula:', 'Here, ‘positive’ is also counted as the count of the positive counts minus count of the number of negatives in the data.', 'In the last step of the process, the two negative counts are combined into one and the counterex is used as the “positive” count. This gives the preprocessing step a count for all the data contained in the data and returns a result that the model can easily exploit. In the methods methods methods.py preprocess, preprocess.py preprocess.py, and preprocess.py, the positive and negative numbers are denoted by numbers. This helps the preprocessing process to focus on the data. The reason that negative numbers often appear in text is because the “positive” and ‘negative” counts are often used interchangeably. Another advantage of using negative numbers for the counterex is that the negative numbers themselves may be helpful in labeling the data. For example, the model might want to identify whether two words are coked up or ‘cuz’ to identify whether the two words are coked up or ‘cuz’ to associate the word ‘cuz’ with\n",
            "\n",
            "[801 | 1898.74] loss=1.57 avg=1.65\n",
            "[802 | 1900.99] loss=1.42 avg=1.64\n",
            "[803 | 1903.24] loss=1.43 avg=1.64\n",
            "[804 | 1905.48] loss=1.28 avg=1.64\n",
            "[805 | 1907.73] loss=1.47 avg=1.64\n",
            "[806 | 1909.97] loss=1.42 avg=1.63\n",
            "[807 | 1912.21] loss=1.55 avg=1.63\n",
            "[808 | 1914.46] loss=1.22 avg=1.63\n",
            "[809 | 1916.71] loss=1.38 avg=1.63\n",
            "[810 | 1918.96] loss=1.31 avg=1.62\n",
            "[811 | 1921.20] loss=0.90 avg=1.62\n",
            "[812 | 1923.45] loss=1.56 avg=1.61\n",
            "[813 | 1925.71] loss=1.36 avg=1.61\n",
            "[814 | 1927.96] loss=1.35 avg=1.61\n",
            "[815 | 1930.20] loss=1.53 avg=1.61\n",
            "[816 | 1932.45] loss=0.97 avg=1.60\n",
            "[817 | 1934.69] loss=1.47 avg=1.60\n",
            "[818 | 1936.94] loss=1.71 avg=1.60\n",
            "[819 | 1939.18] loss=1.35 avg=1.60\n",
            "[820 | 1941.43] loss=1.43 avg=1.60\n",
            "[821 | 1943.68] loss=1.65 avg=1.60\n",
            "[822 | 1945.92] loss=1.50 avg=1.60\n",
            "[823 | 1948.17] loss=1.35 avg=1.60\n",
            "[824 | 1950.42] loss=1.22 avg=1.59\n",
            "[825 | 1952.66] loss=1.58 avg=1.59\n",
            "[826 | 1954.91] loss=1.38 avg=1.59\n",
            "[827 | 1957.16] loss=1.14 avg=1.58\n",
            "[828 | 1959.41] loss=1.51 avg=1.58\n",
            "[829 | 1961.66] loss=1.48 avg=1.58\n",
            "[830 | 1963.91] loss=1.36 avg=1.58\n",
            "[831 | 1966.15] loss=1.25 avg=1.58\n",
            "[832 | 1968.40] loss=1.28 avg=1.57\n",
            "[833 | 1970.65] loss=0.93 avg=1.57\n",
            "[834 | 1972.90] loss=1.17 avg=1.56\n",
            "[835 | 1975.15] loss=0.98 avg=1.56\n",
            "[836 | 1977.40] loss=1.39 avg=1.56\n",
            "[837 | 1979.65] loss=1.00 avg=1.55\n",
            "[838 | 1981.89] loss=1.27 avg=1.55\n",
            "[839 | 1984.15] loss=1.13 avg=1.54\n",
            "[840 | 1986.40] loss=1.33 avg=1.54\n",
            "[841 | 1988.64] loss=1.16 avg=1.54\n",
            "[842 | 1990.89] loss=1.65 avg=1.54\n",
            "[843 | 1993.13] loss=1.55 avg=1.54\n",
            "[844 | 1995.38] loss=1.19 avg=1.54\n",
            "[845 | 1997.63] loss=1.64 avg=1.54\n",
            "[846 | 1999.88] loss=1.49 avg=1.54\n",
            "[847 | 2002.14] loss=1.13 avg=1.53\n",
            "[848 | 2004.39] loss=1.84 avg=1.54\n",
            "[849 | 2006.63] loss=1.37 avg=1.53\n",
            "[850 | 2008.88] loss=1.26 avg=1.53\n",
            "[851 | 2011.13] loss=1.63 avg=1.53\n",
            "[852 | 2013.38] loss=1.28 avg=1.53\n",
            "[853 | 2015.62] loss=1.12 avg=1.53\n",
            "[854 | 2017.87] loss=1.43 avg=1.52\n",
            "[855 | 2020.12] loss=1.46 avg=1.52\n",
            "[856 | 2022.37] loss=1.10 avg=1.52\n",
            "[857 | 2024.62] loss=0.93 avg=1.51\n",
            "[858 | 2026.87] loss=1.63 avg=1.51\n",
            "[859 | 2029.12] loss=1.11 avg=1.51\n",
            "[860 | 2031.37] loss=1.12 avg=1.51\n",
            "[861 | 2033.62] loss=1.08 avg=1.50\n",
            "[862 | 2035.87] loss=1.43 avg=1.50\n",
            "[863 | 2038.11] loss=1.20 avg=1.50\n",
            "[864 | 2040.36] loss=1.31 avg=1.50\n",
            "[865 | 2042.61] loss=1.09 avg=1.49\n",
            "[866 | 2044.85] loss=1.20 avg=1.49\n",
            "[867 | 2047.11] loss=1.26 avg=1.49\n",
            "[868 | 2049.35] loss=1.10 avg=1.48\n",
            "[869 | 2051.60] loss=1.44 avg=1.48\n",
            "[870 | 2053.85] loss=1.27 avg=1.48\n",
            "[871 | 2056.10] loss=1.28 avg=1.48\n",
            "[872 | 2058.34] loss=0.79 avg=1.47\n",
            "[873 | 2060.59] loss=1.34 avg=1.47\n",
            "[874 | 2062.84] loss=1.52 avg=1.47\n",
            "[875 | 2065.09] loss=1.11 avg=1.47\n",
            "[876 | 2067.33] loss=1.29 avg=1.47\n",
            "[877 | 2069.58] loss=1.56 avg=1.47\n",
            "[878 | 2071.83] loss=1.28 avg=1.47\n",
            "[879 | 2074.07] loss=1.28 avg=1.46\n",
            "[880 | 2076.31] loss=1.42 avg=1.46\n",
            "[881 | 2078.55] loss=0.94 avg=1.46\n",
            "[882 | 2080.81] loss=1.01 avg=1.45\n",
            "[883 | 2083.06] loss=0.77 avg=1.45\n",
            "[884 | 2085.31] loss=1.23 avg=1.44\n",
            "[885 | 2087.56] loss=1.35 avg=1.44\n",
            "[886 | 2089.81] loss=1.12 avg=1.44\n",
            "[887 | 2092.07] loss=1.18 avg=1.44\n",
            "[888 | 2094.32] loss=1.24 avg=1.44\n",
            "[889 | 2096.57] loss=1.05 avg=1.43\n",
            "[890 | 2098.82] loss=1.07 avg=1.43\n",
            "[891 | 2101.07] loss=1.38 avg=1.43\n",
            "[892 | 2103.31] loss=1.19 avg=1.43\n",
            "[893 | 2105.56] loss=1.18 avg=1.42\n",
            "[894 | 2107.81] loss=1.19 avg=1.42\n",
            "[895 | 2110.05] loss=1.20 avg=1.42\n",
            "[896 | 2112.31] loss=1.06 avg=1.41\n",
            "[897 | 2114.56] loss=1.15 avg=1.41\n",
            "[898 | 2116.81] loss=1.02 avg=1.41\n",
            "[899 | 2119.06] loss=1.54 avg=1.41\n",
            "[900 | 2121.31] loss=1.17 avg=1.41\n",
            "======== SAMPLE 1 ========\n",
            " converses-bertes, these questions are the ones that “Can you predict a user” and “Can you predict a product”?” The following is an example of a question answering query which is posed by “Which restaurant has the best food?”', 'Note that you can also ask questions to find more details about the user, such as their name, social media profile etc.', 'Butler’s Notebook', 'When using AWEs, we often create context-wrapped models by putting them in situations where “the user doesn’t have access to what is called “context.” By putting “context” in the query, we can keep the context that will allow the model to predict the user “is” there as well.', 'For example, how can we predict whether an incoming message is from “Mr. Trump” or “Mr. Xi”? The context-wrapping AWE model can then be chained to these predictions to create the desired “sense” of brand awareness.', '“Who is the most influential person on your market?”', 'If you have ever watched a football game, the user would be pretty unlikely to know that a potentially influential person is the football player”s name, if they’re not already.', 'And this, I think, is why brand awareness models are still in use. Models that really question the user’s perception of brands. AWEs are nothing new, they’ve been around for quite some time. Back in the day you could build a custom-made model- on your own using your own data-points, or even as a pre-trained model from a specific dataset — the choice is pretty much up to the model makers themselves. AWEs are still relatively new, they’ve only been trained at BERT, and they’ll probably get much higher validation scores in the future. As a reminder, a model that does a SINGLE statement at a time — AWE model.', 'But I do plan to keep on sharing more about the AWE language. I worked on it at my desk for the AWE community, and in the end got great results! Check it out:', 'Here’s an example that I made on a conference call to discuss our new emoji speech recognition API :', 'Here’s an example of a brand awareness model being developed:', 'With all of the recent AWE successes and new data coming in, it’s important for us as AWE developers to scale and market our initiatives. It’s not easy, and it’s certainly isn’t free, so there’s a fair amount of scope in this post to go into that.', 'Up until now, AWE models were either trained on tweets or self-supervised tasks like they did in the Women In Analytics post.', 'Now, we have both high-level and non-level engagements, where the objective is to predict whether an engagement is necessary in order to achieve an outcome (for example, “I’m the one initiating the engagement” means I initiate the engagement). In order to effectively harness the power of AWEs, we need to assess how these engagements affect the modelling process. Ideally, we could have AWE models deployed for engagements and manually supervised modelling, but that would be too much, and in the interest of transparency, I won’t be discussing that here. Instead, I’ve created a simple “configure” script which dynamically scales and configures AWE models on all my models. Note that you can read the entire config here!', 'In the config, you can either specify the number of models to be deployed or the maximum number of models to be deployed by the API. You should ideally be able to fit hundreds of thousands of traffic tickets a month. Here’s an example of how the model setup looks like :', 'Here the first column (the user) is the number of models to be deployed, and the second column (the number of engagements) contains the engagement_description column which describes the engagement description of the model.', 'The config file can be found here, and here I have mapped the model number and model_description column from the engagements to the ones they will have in the API. As you may have guessed, this will impact performance quite a bit. This is, however, entirely configurable. However, I wanted to create something very very clean — meaning keeping only the models that were defined earlier in the config.', 'This means I have a config file for each document in which you will need to specify the model number, model_description, and model_amount columns. To sum it up, I have set a number of config variables that define the model parameters.', '\n",
            "\n",
            "[901 | 2135.06] loss=1.56 avg=1.41\n",
            "[902 | 2137.30] loss=1.18 avg=1.41\n",
            "[903 | 2139.55] loss=1.00 avg=1.40\n",
            "[904 | 2141.80] loss=1.20 avg=1.40\n",
            "[905 | 2144.05] loss=0.89 avg=1.40\n",
            "[906 | 2146.30] loss=1.51 avg=1.40\n",
            "[907 | 2148.54] loss=1.37 avg=1.40\n",
            "[908 | 2150.78] loss=1.40 avg=1.40\n",
            "[909 | 2153.02] loss=1.15 avg=1.39\n",
            "[910 | 2155.27] loss=1.18 avg=1.39\n",
            "[911 | 2157.52] loss=1.36 avg=1.39\n",
            "[912 | 2159.77] loss=1.56 avg=1.39\n",
            "[913 | 2162.01] loss=1.19 avg=1.39\n",
            "[914 | 2164.27] loss=1.27 avg=1.39\n",
            "[915 | 2166.52] loss=1.54 avg=1.39\n",
            "[916 | 2168.77] loss=1.26 avg=1.39\n",
            "[917 | 2171.01] loss=1.28 avg=1.39\n",
            "[918 | 2173.26] loss=1.11 avg=1.39\n",
            "[919 | 2175.51] loss=0.95 avg=1.38\n",
            "[920 | 2177.76] loss=1.02 avg=1.38\n",
            "[921 | 2180.00] loss=1.71 avg=1.38\n",
            "[922 | 2182.25] loss=0.81 avg=1.38\n",
            "[923 | 2184.50] loss=0.99 avg=1.37\n",
            "[924 | 2186.74] loss=1.45 avg=1.37\n",
            "[925 | 2188.98] loss=1.87 avg=1.38\n",
            "[926 | 2191.23] loss=1.07 avg=1.37\n",
            "[927 | 2193.48] loss=1.09 avg=1.37\n",
            "[928 | 2195.73] loss=1.04 avg=1.37\n",
            "[929 | 2197.97] loss=1.50 avg=1.37\n",
            "[930 | 2200.21] loss=1.17 avg=1.37\n",
            "[931 | 2202.46] loss=1.44 avg=1.37\n",
            "[932 | 2204.70] loss=0.88 avg=1.36\n",
            "[933 | 2206.94] loss=1.32 avg=1.36\n",
            "[934 | 2209.19] loss=1.12 avg=1.36\n",
            "[935 | 2211.43] loss=1.02 avg=1.36\n",
            "[936 | 2213.67] loss=1.12 avg=1.35\n",
            "[937 | 2215.92] loss=1.15 avg=1.35\n",
            "[938 | 2218.17] loss=1.69 avg=1.36\n",
            "[939 | 2220.43] loss=1.16 avg=1.35\n",
            "[940 | 2222.68] loss=0.90 avg=1.35\n",
            "[941 | 2224.92] loss=1.60 avg=1.35\n",
            "[942 | 2227.17] loss=0.98 avg=1.35\n",
            "[943 | 2229.41] loss=1.18 avg=1.35\n",
            "[944 | 2231.66] loss=1.26 avg=1.35\n",
            "[945 | 2233.91] loss=1.05 avg=1.34\n",
            "[946 | 2236.16] loss=1.12 avg=1.34\n",
            "[947 | 2238.41] loss=1.04 avg=1.34\n",
            "[948 | 2240.65] loss=0.92 avg=1.33\n",
            "[949 | 2242.90] loss=1.15 avg=1.33\n",
            "[950 | 2245.15] loss=0.88 avg=1.33\n",
            "[951 | 2247.39] loss=1.36 avg=1.33\n",
            "[952 | 2249.63] loss=0.87 avg=1.32\n",
            "[953 | 2251.88] loss=1.18 avg=1.32\n",
            "[954 | 2254.12] loss=1.28 avg=1.32\n",
            "[955 | 2256.38] loss=1.07 avg=1.32\n",
            "[956 | 2258.63] loss=1.15 avg=1.32\n",
            "[957 | 2260.87] loss=0.90 avg=1.31\n",
            "[958 | 2263.12] loss=1.33 avg=1.31\n",
            "[959 | 2265.36] loss=1.48 avg=1.31\n",
            "[960 | 2267.61] loss=1.13 avg=1.31\n",
            "[961 | 2269.85] loss=1.42 avg=1.31\n",
            "[962 | 2272.10] loss=0.93 avg=1.31\n",
            "[963 | 2274.35] loss=0.92 avg=1.31\n",
            "[964 | 2276.60] loss=1.33 avg=1.31\n",
            "[965 | 2278.85] loss=1.48 avg=1.31\n",
            "[966 | 2281.09] loss=1.24 avg=1.31\n",
            "[967 | 2283.34] loss=0.98 avg=1.30\n",
            "[968 | 2285.59] loss=0.93 avg=1.30\n",
            "[969 | 2287.84] loss=1.00 avg=1.30\n",
            "[970 | 2290.08] loss=1.40 avg=1.30\n",
            "[971 | 2292.33] loss=0.92 avg=1.29\n",
            "[972 | 2294.58] loss=0.96 avg=1.29\n",
            "[973 | 2296.83] loss=1.02 avg=1.29\n",
            "[974 | 2299.08] loss=1.00 avg=1.29\n",
            "[975 | 2301.32] loss=1.08 avg=1.28\n",
            "[976 | 2303.57] loss=0.55 avg=1.28\n",
            "[977 | 2305.82] loss=1.03 avg=1.27\n",
            "[978 | 2308.07] loss=0.95 avg=1.27\n",
            "[979 | 2310.32] loss=0.98 avg=1.27\n",
            "[980 | 2312.57] loss=1.58 avg=1.27\n",
            "[981 | 2314.82] loss=1.06 avg=1.27\n",
            "[982 | 2317.06] loss=0.82 avg=1.26\n",
            "[983 | 2319.31] loss=1.00 avg=1.26\n",
            "[984 | 2321.56] loss=0.79 avg=1.26\n",
            "[985 | 2323.80] loss=0.97 avg=1.25\n",
            "[986 | 2326.06] loss=1.45 avg=1.26\n",
            "[987 | 2328.31] loss=0.92 avg=1.25\n",
            "[988 | 2330.55] loss=1.01 avg=1.25\n",
            "[989 | 2332.80] loss=0.80 avg=1.25\n",
            "[990 | 2335.05] loss=1.19 avg=1.25\n",
            "[991 | 2337.30] loss=1.52 avg=1.25\n",
            "[992 | 2339.54] loss=1.25 avg=1.25\n",
            "[993 | 2341.79] loss=1.29 avg=1.25\n",
            "[994 | 2344.04] loss=0.66 avg=1.24\n",
            "[995 | 2346.29] loss=0.84 avg=1.24\n",
            "[996 | 2348.54] loss=1.19 avg=1.24\n",
            "[997 | 2350.79] loss=0.86 avg=1.23\n",
            "[998 | 2353.04] loss=0.74 avg=1.23\n",
            "[999 | 2355.29] loss=0.80 avg=1.22\n",
            "[1000 | 2357.53] loss=1.01 avg=1.22\n",
            "Saving checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhSSOkPMj1Ad",
        "outputId": "f9b3da5f-7422-481b-acd2-a7fcb47d0e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Loading checkpoint checkpoint/run1/model-100\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNX5KETF5kle",
        "outputId": "170e3ef0-d3a6-460d-9d14-04e7c1a6bb40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "gpt2.generate(sess, run_name='run3')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ") (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n",
            "  (0.5m)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O97vJWVqp6eN",
        "outputId": "959b3ec2-467b-4474-c4cf-d800a07fb81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "gpt2.generate(sess,prefix='natural language',top_k=40,length=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "natural language has been a source of confusion in recent years. One of the first steps in becoming a better linguist was to first try to calculate the difference in the different varieties of a different sign for every different sign. One way to do this was to calculate the sign differences for each sign.\n",
            "\n",
            "To do this, I first calculate the sign differences for different sign combinations. For example, if I say \"say hello to a dog\" I calculate that sign differences for both sign sets. For a different\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avJnmE7So0bn",
        "outputId": "5a6751a1-9cf3-4987-a516-b10f20f11b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "single_text = gpt2.generate(sess, return_as_list=True)[0]\n",
        "print(single_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "If you do not already own a phone or tablet, a free Google Account (https://accounts.google.com/signin/web/usernamecard) or sign-in with a different account account (https://accounts.google.com/SignIn?loc=US&hl=en), you do not have to do any of the steps outlined in this article.\n",
            "\n",
            "If you do not already own a phone or tablet, a free Google Account (https://accounts.google.com/signin/web/usernamecard) or sign-in with a different account account (https://accounts.google.com/SignIn?loc=US&hl=en), you do not have to do any of the steps outlined in this article.\n",
            "\n",
            "In this article\n",
            "\n",
            "If you do not already own a phone or tablet, a free Google Account (https://accounts.google.com/signin/web/usernamecard) or sign-in with a different account account (https://accounts.google.com/SignIn?loc=US&hl=en)\n",
            "\n",
            "If you do not already own a phone or tablet, a free Google Account (https://accounts.google.com/signin/web/usernamecard) or sign-in with a different account account (https://accounts.google.com/SignIn?loc=US&hl=en)\n",
            "\n",
            "If you do not already own a phone or tablet, a free Google Account (https://accounts.google.com/signin/web/usernamecard) or sign-in with a different account account (https://accounts.google.com/SignIn?loc=US&hl=en)\n",
            "\n",
            "If you do not already own a phone or tablet, a free Google Account (https://accounts.google.com/signin/web/usernamecard) or sign-in with a different account account (https://accounts.google.com/SignIn?loc=US&hl=en)\n",
            "\n",
            "If you do not already own a phone or tablet, a free Google Account (https://accounts.google.com/signin/web/usernamecard) or sign-in with a different account account (https://accounts.google.com/SignIn?loc=US&hl=en)\n",
            "\n",
            "If you do not already own a phone or tablet, and if you do not want to continue to sign in with a different account account, sign-in with a different phone number (for example, 800-537-7899), sign in with a different email account or email account from Google Drive (for Create Account), sign in with a different phone number (for example, 800-537-7899), sign out with a different email account or email account from Google Drive (for Create Account), sign in with a different email account or email account from Google Drive (for Create Account), sign out with a different phone number (for example, 800-537-7899), sign in with an account with different phone numbers (for example, 800-537-7899), sign in with a different phone number (for example, 800-537-7899), sign in with a different phone number (for example, 800-537-7899), sign in with an account with different phone numbers (for example, 800-537-7899), sign in with an account with different phone numbers (for example, 800-537-7899), sign in with an account with different phone numbers (for example, 800-537-7899), sign in with an account with different phone numbers (for example, 800-537-7899), sign out with a different phone number (for example, 800-537-7899), sign in with an account with different phone numbers (for example, 800-537-7899), sign out with an account with different phone numbers (for example, 800-537-7899), sign in with an account with different phone numbers (for example, 800-537-7899), sign in with an account with different phone numbers (for example, 800-537-7899), sign in with an account with different phone numbers (for a different phone number), sign in with an account number (for a different phone number), sign in with an account number (for a different phone number), sign in with a valid Google Account (https://accounts.google.com/support/accounts?hl=en)\n",
            "\n",
            "If you do not already own a phone or tablet, a free Google Account (https://accounts.google.com/signin/web/usernamecard) or sign-in with a different account account (https://accounts.google.com/SignIn?loc=US&hl=en), you do not have to do any of the steps outlined in this article.\n",
            "\n",
            "If you do not already own a phone or tablet, a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsUUFTV9nT7h"
      },
      "source": [
        "!cp -r /content/checkpoint/run1 /content/drive/My\\ Drive/run1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHGzQ_KinngI",
        "outputId": "967a625f-1ee7-44bd-d344-828742e76262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"running still\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running still\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l57rbxILBMLk"
      },
      "source": [
        "!cp -r /content/checkpoint/run3/ /content/drive/My\\ Drive/run_3/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}